{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative_vs_discriminative.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/GenerativeVsDiscriminative/blob/master/ipynb/generative_vs_discriminative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpggye9TxS-s",
        "colab_type": "text"
      },
      "source": [
        "# Generative *vs* Discriminative models for inference\n",
        "\n",
        "- Do generative models generally outperform discriminative models in various ways?\n",
        "- How do both of these compare to information-theory-optimal estimators?\n",
        "- Are generative models less subject to adversarial attack?\n",
        "- Are generative and discriminative models equally good for de-noising noisy labels?\n",
        "- ... and related matters.\n",
        "\n",
        "## authors\n",
        "- **David W Hogg** *(NYU) (MPIA) (Flatiron)*\n",
        "- **Soledad Villar** *(NYU)*\n",
        "\n",
        "## license\n",
        "Copyright 2019, 2020 the authors. All rights reserved (for now).\n",
        "\n",
        "## notes and to-do items\n",
        "- Replace all `pseudo_solve()` and `pseudo_inverse()` calls with `np.linalg.lstsq()[0]` calls.\n",
        "- Consider making a method that fits a Gaussian to the joint of `X` and `Y`.\n",
        "- Rearrange things so we can loop over some amplitudes of `C_x` and `C_y` as well.\n",
        "- Decide what are our main results / observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLXl4Vx8WYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "#matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYQYdl_g8t-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the data-generating matrices of God\n",
        "# BUG: This should probably be a function call.\n",
        "\n",
        "# set God's integers\n",
        "maximalD = 2 ** 9 # ESLII calls this \"p\"\n",
        "fiducialD = 2 ** 7\n",
        "M = 1 # ESLII always sets this to 1\n",
        "K = 2 ** 4\n",
        "assert(M <= K)\n",
        "\n",
        "# set training set size\n",
        "maximalN = maximalD\n",
        "fiducialN = 2 ** 6\n",
        "Ntest = 2 ** 8 # number of points to use to compute biases and variances\n",
        "Ntrial = 2 ** 4 # number of training trials to use to compute biases and variances\n",
        "\n",
        "# set God's matrices\n",
        "# the true world can be a (K-1)-order Legendre (orthogonal) or Chebyshev (non-orthogonal) polynomial\n",
        "#Q = np.vstack([np.polynomial.legendre.legval(np.arange(-1. + 1./D, 1., 2./D),\n",
        "#                                             np.eye(K)[k])\n",
        "#    for k in range(K)]).T\n",
        "# or the true world can be a set of random matrices\n",
        "np.random.seed(42)\n",
        "maximalQ = np.random.normal(size=(maximalD, K))\n",
        "np.random.seed(17)\n",
        "P = np.random.normal(size=(M, K))\n",
        "\n",
        "# make noise covariance matrices\n",
        "np.random.seed(13)\n",
        "foo = np.random.normal(size=(maximalD, 2 * maximalD))\n",
        "maximalCx = 2 ** -2 * foo @ foo.T\n",
        "foo = np.random.normal(size=(M, 2 * M))\n",
        "Cy = 2 ** -2 * foo @ foo.T\n",
        "foo = np.random.normal(size=(K, 2 * K))\n",
        "V = foo @ foo.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE9CzcvwkjdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define functions to take and use pseudo-inverses.\n",
        "\n",
        "def pseudo_solve(A, x, Cinv=None, weights=None, rcond=None, lamb=0.):\n",
        "  \"\"\"\n",
        "  return solve(ATA, ATx) but be somewhat clever about numerics\n",
        "\n",
        "  ## inputs:\n",
        "  - A: rectangular matrix\n",
        "  - Cinv (optional): square non-negative semi-definite matrix that can multiply\n",
        "      AT on the right or A on the left, and x on the left.\n",
        "  - weight (optional): diagonal of Cinv if Cinv is diagonal (conflicts with\n",
        "      Cinv).\n",
        "  - rcond (optional): a regularization parameter for np.linalg.lstsq().\n",
        "  - lamb (optional): L2 regularization parameter (use at your own risk).\n",
        "\n",
        "  ## outputs:\n",
        "  - Pseudo-inverse of A applied to x.\n",
        "  \"\"\"\n",
        "  foo, bar = A.shape # need this to decide whether to do ATA or AAT\n",
        "\n",
        "  # use weights or Cinv to weight A.T\n",
        "  if weights is not None:\n",
        "    assert Cinv is None, \"pseudo_inverse(): You can't set both Cinv and weight\"\n",
        "    wAT = weights * A.T\n",
        "  elif Cinv is not None:\n",
        "    wAT = A.T @ Cinv\n",
        "  else:\n",
        "    wAT = A.T\n",
        "\n",
        "  if foo < bar:\n",
        "    AwAT = A @ wAT\n",
        "    if x is None:\n",
        "      return np.linalg.lstsq(AwAT + lamb * np.eye(foo), wAT.T, rcond=rcond)[0].T\n",
        "    return wAT @ np.linalg.lstsq(AwAT + lamb * np.eye(foo), x, rcond=rcond)[0] # There are many ways to write this line\n",
        "\n",
        "  if x is None:\n",
        "    wATx = wAT\n",
        "  else:\n",
        "    wATx = wAT @ x\n",
        "\n",
        "  wATA = wAT @ A\n",
        "  return np.linalg.lstsq(wATA + lamb * np.eye(bar), wATx, rcond=None)[0]\n",
        "\n",
        "def pseudo_inverse(A, Cinv=None, weights=None, rcond=None, lamb=0.):\n",
        "  \"\"\"\n",
        "  return solve(ATA, AT) but be somewhat clever about numerics\n",
        "  \"\"\"\n",
        "  return pseudo_solve(A, None, Cinv=Cinv, weights=weights, rcond=rcond, lamb=lamb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uci_lRUKkatw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gods_estimator(P, Q, V, Cx):\n",
        "  \"\"\"\n",
        "  make the best possible estimator, in terms of mean squared error on Y\n",
        "  (this, we hope, is what everything approaches, in the limit N -> infinity)\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Q @ V @ Q.T + Cx, Q @ V @ P.T,\n",
        "                         rcond=None)[0].T\n",
        "  \"\"\"\n",
        "  # OLD STUFF\n",
        "  # BUGS:\n",
        "  # - REQUIRES M=1.\n",
        "  D, K = Q.shape\n",
        "  M, KK = P.shape\n",
        "  assert K == KK\n",
        "  assert M == 1 # holy mother of God\n",
        "  print(np.diag(1. / xivars).shape, Q.shape, Q.T.shape, np.zeros((K, K)).shape)\n",
        "  foo = np.vstack((np.hstack((np.diag(1. / xivars), Q)),\n",
        "                   np.hstack((Q.T, np.zeros((K, K))))))\n",
        "  bar = np.append(np.zeros(D), P.flatten())\n",
        "  bar = np.linalg.lstsq(foo, bar, rcond=None)[0]\n",
        "  return bar[:D].reshape((D, M)).T\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7z7OH--1MSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maximalW = gods_estimator(P, maximalQ, V, maximalCx)\n",
        "maximalW.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUGYRRYKlFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data_set(N, P, Q, V, Cx, Cy):\n",
        "  \"\"\"\n",
        "  actually make the random data, using the matrices of God\n",
        "  \"\"\"\n",
        "  M, K = P.shape\n",
        "  D, KK = Q.shape\n",
        "  assert K == KK\n",
        "  zs = np.random.multivariate_normal(np.zeros(K), V, size=N)\n",
        "  godxs = (Q @ zs.T).T # \"true\" xs\n",
        "  xs = godxs + np.random.multivariate_normal(np.zeros(D), Cx, size=N)\n",
        "  godys = (P @ zs.T).T # \"true\" ys\n",
        "  ys = godys + np.random.multivariate_normal(np.zeros(M), Cy, size=N)\n",
        "  return xs, ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggWSWinWMD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make maximal train and test sets\n",
        "# NOTE THAT THE TEST Y HAS INFINITE yivars OR ZERO NOISE\n",
        "np.random.seed(8675309 + 1)\n",
        "maximalX = np.zeros((Ntrial, maximalN, maximalD))\n",
        "maximalY = np.zeros((Ntrial, maximalN, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalX[trial], maximalY[trial] = make_data_set(maximalN, P, maximalQ, V, maximalCx, Cy)\n",
        "maximalXtest = np.zeros((Ntrial, Ntest, maximalD))\n",
        "maximalYtest = np.zeros((Ntrial, Ntest, M))\n",
        "for trial in range(Ntrial):\n",
        "  # zero-noise test labels maximalYtest\n",
        "  maximalXtest[trial], maximalYtest[trial] = make_data_set(Ntest, P, maximalQ, V, maximalCx, 0. * Cy)\n",
        "print(maximalX.shape, maximalY.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI_hm117kftA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u, s, v = np.linalg.svd(maximalX[0, :fiducialN, :fiducialD], full_matrices=False)\n",
        "plt.axvline(K)\n",
        "plt.plot(s, \"ko\")\n",
        "plt.semilogy()\n",
        "plt.ylabel(\"contribution to variance\")\n",
        "plt.xlabel(\"component number\")\n",
        "plt.title(\"SVD of the fiducial data set, zeroth trial\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGrRc7kO8-8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#examples = np.arange(4)\n",
        "#colors=['red', 'purple', 'green', 'blue']\n",
        "#for i,n in enumerate(examples):\n",
        "#  plt.step(np.arange(maximalD), xs_train[n], linestyle='-',alpha=0.5, color=colors[i], where=\"mid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjsOAJNShEdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminative_model(xs, ys):\n",
        "  \"\"\"\n",
        "  train discriminative model y = B x + noise\n",
        "  \n",
        "  ## inputs:\n",
        "  - xs - array of training data\n",
        "  - ys - array of training labels\n",
        "\n",
        "  ## comments\n",
        "  Informally speaking, this finds the B that minimizes || ys - B . xs || (plus\n",
        "  some regularization), and returns the discriminative matrix B.\n",
        "\n",
        "  ## bugs:\n",
        "  - Properly, this should take in an estimate of the inverse variances. These\n",
        "    might be some processing of the xivars and yivars; we need to figure that\n",
        "    out.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(xs, ys, rcond=None)[0].T\n",
        "\n",
        "def train_trivial_generative_model(xs, ys):\n",
        "  \"\"\"\n",
        "  say stuff here\n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(ys, xs, rcond=None)[0]\n",
        "\n",
        "def initialize_a_generative_model(Y, P):\n",
        "  Z = np.linalg.lstsq(P, Y.T, rcond=None)[0]\n",
        "  P1 = P.T\n",
        "  proj_P = P1 @ pseudo_inverse(P1)\n",
        "  M, humanK = P.shape\n",
        "  proj_Pperp = np.identity(humanK) - proj_P\n",
        "  return proj_P @ Z, proj_Pperp @ np.random.normal(size=Z.shape), proj_Pperp\n",
        "\n",
        "def train_msv_generative_model(X, Y, P, maxiter=10000, Z=None):\n",
        "  \"\"\"\n",
        "  ## Inputs:\n",
        "  X: N x D\n",
        "  Y: N x M\n",
        "  P: M x humanK\n",
        "\n",
        "  ## Comments:\n",
        "  among all solutions Z such that Y=Z*P.T (ie Y.T=P*Z.T)\n",
        "  we minimize ||X - Z*A|| by alternating minimization\n",
        "  we write Z = Z0 + Z1, proj_P*Z=Z0, proj_Pperp*Z = Z1 \n",
        "\n",
        "  ## bugs:\n",
        "  - Can be simplified with pseudo_inverse().\n",
        "  - Doesn't use xivars, yivars.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  M, humanK = P.shape\n",
        "  \n",
        "  Z0, Z1, proj_Pperp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = Z0 + Z1\n",
        "  tiny = 1.e-9\n",
        "  unconverged = True\n",
        "  last_cost=np.inf\n",
        "  for i in range(maxiter):\n",
        "    # normalize the perpendicular part of Z\n",
        "    Z1 = proj_Pperp @ Z\n",
        "    Z -= Z1\n",
        "    Z1 /= tiny + np.sqrt(np.mean(Z1 ** 2, axis=1))[:, None]\n",
        "    Z += Z1\n",
        "\n",
        "    #fix Z optimize for A\n",
        "    A=np.linalg.lstsq(Z.T, X, rcond=None)[0]\n",
        "\n",
        "    #fix A optimize for Z\n",
        "    a = A.T @ proj_Pperp.T\n",
        "    b = X.T - A.T @ Z0\n",
        "    Z1 = np.linalg.lstsq(a, b, rcond=None)[0]\n",
        "    Z1 = proj_Pperp @ Z1\n",
        "\n",
        "    #convergence\n",
        "    Z_old = Z\n",
        "    Z = Z0 + Z1\n",
        "    #BUG: unstable! \n",
        "    #print('convergence, cost')\n",
        "    cost= np.linalg.norm(X.T - A.T @ Z)\n",
        "\n",
        "    if cost >= last_cost:\n",
        "      unconverged = False\n",
        "      break\n",
        "    last_cost = cost\n",
        "    #print((np.linalg.norm(Z-Z_old), np.linalg.norm(X.T- np.matmul(A.T,Z)) + np.linalg.norm(Y.T -np.matmul(P,Z)) ))\n",
        "  if unconverged: print(\"train_msv_generative_model(): WARNING: did not converge.\")\n",
        "  return  P @ pseudo_inverse(A).T, Z.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rgWZgx0Kjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  bigA = np.vstack((sqrtCxinv @ A, sqrtCyinv @ P))\n",
        "  bigX = np.hstack((X @ sqrtCxinv, Y @ sqrtCyinv))\n",
        "  return np.linalg.lstsq(bigA, bigX.T, rcond=None)[0].T\n",
        "\n",
        "def a_step(X, Z):\n",
        "  \"\"\"\n",
        "  BUG: This doesn't currently use the Cx, Cy correctly.\n",
        "  BUG: meaning: It treats the data as homoskedastic.\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Z, X, rcond=None)[0].T\n",
        "\n",
        "def dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Untested!\n",
        "  \"\"\"\n",
        "  xchi = sqrtCxinv @ (X.T - (A @ Z.T))\n",
        "  ychi = sqrtCyinv @ (Y.T - (P @ Z.T))\n",
        "  return np.sum(xchi ** 2) + np.sum(ychi ** 2)\n",
        "\n",
        "def train_dwh_generative_model(X, Y, P, Cx, Cy, maxiter=10000, lltol=0.00001, Z=None):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Initialization bad here. This should probably initialize at the internal\n",
        "    state of the MSV generative model optimized earlier.\n",
        "  - Takes forever to converge when K = M, apparently. This may be related to\n",
        "    initialization also.\n",
        "  \"\"\"\n",
        "  # get read the square roots of the inverses of Cx, Cy\n",
        "  u, s, v = np.linalg.svd(Cx)\n",
        "  sqrtCxinv = (u / np.sqrt(s)) @ v\n",
        "  Cxinv = (u / s) @ v\n",
        "  u, s, v = np.linalg.svd(Cy)\n",
        "  sqrtCyinv = (u / np.sqrt(s)) @ v\n",
        "\n",
        "  # initialize loop\n",
        "  Z0, Z1, proj_P_perp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = (Z0 + Z1).T\n",
        "  tiny = 1.e-9\n",
        "  cost = np.inf\n",
        "  unconverged = True\n",
        "  A = 0.\n",
        "\n",
        "  # loop\n",
        "  for ii in range(maxiter):\n",
        "    old_cost = 1. * cost\n",
        "    oldA = 1. * A\n",
        "\n",
        "    # normalization step\n",
        "    Zperp = (proj_P_perp @ Z.T).T\n",
        "    Z -= Zperp\n",
        "    Zperp /= tiny + np.sqrt(np.mean(Zperp ** 2, axis=0))[None, :]\n",
        "    Z += Zperp\n",
        "\n",
        "    # standard iteration\n",
        "    A = a_step(X, Z)\n",
        "    Z = z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv)\n",
        "\n",
        "    # convergence control\n",
        "    cost = dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv)\n",
        "    if cost > 1. and cost > (old_cost + lltol): # note insane condition\n",
        "      print(\"train_dwh_generative_model(): WARNING: cost went the wrong way!\")\n",
        "      print(ii, cost, old_cost)\n",
        "      A = oldA # restore\n",
        "    if cost > (old_cost - lltol):\n",
        "      unconverged = False\n",
        "      break\n",
        "  if unconverged: print(\"train_dwh_generative_model(): WARNING: did not converge.\")\n",
        "  return P @ pseudo_inverse(A, Cinv=Cxinv), Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNiam-vuF-vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run on the first trial case, for the fiducials\n",
        "xs_train = maximalX[0, :fiducialN, :fiducialD]\n",
        "ys_train = maximalY[0, :fiducialN, :]\n",
        "xs_test = maximalXtest[0, :, :fiducialD]\n",
        "ys_test = maximalYtest[0, :, :]\n",
        "Cx = maximalCx[:fiducialD, :fiducialD]\n",
        "W = gods_estimator(P, maximalQ[:fiducialD, :], V, Cx)\n",
        "\n",
        "print(xs_train.shape, ys_train.shape)\n",
        "B = train_discriminative_model(xs_train, ys_train)\n",
        "Hdagger = train_trivial_generative_model(xs_train, ys_train)\n",
        "print(B.shape, Hdagger.shape)\n",
        "\n",
        "# humanP = P\n",
        "# humanP = np.hstack((P, np.zeros((M, 2))))\n",
        "# humanP = P[:, :9]\n",
        "humanK = K # stupid\n",
        "assert humanK >= M\n",
        "humanP = np.eye(humanK)[:M, :]\n",
        "print(P.shape, humanP.shape)\n",
        "G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "# hack: Use MSV's latents to initialize DWH's model\n",
        "G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "\n",
        "ys_efficient_test = (W @ xs_test.T).T #W is information theory optimal\n",
        "ys_msv_test = (G_msv @ xs_test.T).T\n",
        "ys_dwh_test = (G_dwh @ xs_test.T).T\n",
        "ys_discriminative_test = (B @ xs_test.T).T\n",
        "ys_trivial_test = (Hdagger @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpSfFNVabyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rms(x):\n",
        "  return np.sqrt(np.mean(x * x))\n",
        "\n",
        "m = 0\n",
        "print(\"best possible\", rms(ys_test[:, m] - ys_efficient_test[:, m]))\n",
        "print(\"discriminative\", rms(ys_test[:, m] - ys_discriminative_test[:, m]))\n",
        "print(\"alt-trivial generative\", rms(ys_test[:, m] - ys_trivial_test[:, m]))\n",
        "print(\"MSV generative\", rms(ys_test[:, m] - ys_msv_test[:, m]))\n",
        "print(\"DWH generative\", rms(ys_test[:, m] - ys_dwh_test[:, m]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYR1db7byM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(151)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_discriminative_test[:, m], bins=32)\n",
        "plt.title(\"discriminative\")\n",
        "plt.subplot(155)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_efficient_test[:, m], bins=bins)\n",
        "plt.title(\"God's best\")\n",
        "plt.subplot(152)\n",
        "plt.hist(ys_test[:, m] - ys_trivial_test[:, m], bins=bins)\n",
        "plt.title(\"alt-trivial generative\")\n",
        "plt.subplot(153)\n",
        "plt.hist(ys_test[:, m] - ys_msv_test[:, m], bins=bins)\n",
        "plt.title(\"MSV generative\")\n",
        "plt.subplot(154)\n",
        "plt.hist(ys_test[:, m] - ys_dwh_test[:, m], bins=bins)\n",
        "plt.title(\"DWH generative\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6b_5uRdsye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mean_variance(ixs):\n",
        "  xs = np.array(ixs)\n",
        "  mean = np.mean(xs)\n",
        "  var = np.mean(xs ** 2) - mean ** 2\n",
        "  return mean, var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjZub21_eoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make D plot at fiducialN\n",
        "\n",
        "humanK = K # stupid\n",
        "N = fiducialN\n",
        "Ds = np.round((maximalD * 2 ** np.arange(-6.5, 0.01, 0.25))).astype(int)\n",
        "dys_D = np.zeros((len(Ds), 4, Ntrial, Ntest, M))\n",
        "Ws, Bs, Gs_msv, Gs_dwh = [], [], [], []\n",
        "for ii, D in enumerate(Ds):\n",
        "  print(\"starting trials for D =\", D)\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  Ws.append(W)\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    if trial == 0:\n",
        "      Bs.append(B)\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    if trial == 0:\n",
        "      Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    if trial == 0:\n",
        "      Gs_dwh.append(G_dwh)\n",
        "    dys_D[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_D[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_D[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_D[ii, 3, trial] = ys_test - (W @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcBpKpa_exu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_D = np.mean(np.mean(np.mean(dys_D ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.min(mses_D)\n",
        "y2 = np.max(mses_D) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ds, mses_D[:, 3], y1 + 0. * mses_D[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ds, mses_D[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ds, mses_D[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ds, mses_D[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(Ds, mses_D[:, 3], \"k-\", lw=0.5, label=\"best possible\")\n",
        "biases = [(W @ maximalQ[:D, :] - P).flatten() for D, W in zip(Ds, Ws)]\n",
        "foo = [np.dot(bias, bias) for bias in biases]\n",
        "# plt.plot(Ds, variances, \"r-\", lw=0.5, label=\"expected variances\")\n",
        "plt.plot(Ds, foo, \"b-\", lw=0.5, label=\"bias of the best\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K = humanK\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of TRUE y\")\n",
        "plt.xlabel(\"number of image pixels D\")\n",
        "plt.title(\"varying D at fixed N, K, P, C_y; truncating or extending Q, C_x\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH_-5JZpcFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make N plot at fiducialD\n",
        "\n",
        "humanK = K # stupid\n",
        "D = fiducialD\n",
        "Ns = np.round((maximalN * 2 ** np.arange(-6., 0.01, 0.25))).astype(int)\n",
        "dys_N = np.zeros((len(Ns), 4, Ntrial, Ntest, M))\n",
        "for ii, N in enumerate(Ns):\n",
        "  print(\"starting trials for N =\", N)\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    Bs.append(B)\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    Gs_dwh.append(G_dwh)\n",
        "    # re-run as a test\n",
        "    # G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP, Z=zs.T)\n",
        "    dys_N[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_N[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_N[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_N[ii, 3, trial] = ys_test - (W @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSz9OKd1Yjq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_N = np.mean(np.mean(np.mean(dys_N ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.min(mses_N)\n",
        "y2 = np.max(mses_N) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ns, mses_N[:, 3], y1 + 0. * mses_N[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ns, mses_N[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ns, mses_N[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ns, mses_N[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(Ns, mses_N[:, 3], \"k-\", lw=0.5, label=\"best possible\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K = humanK\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of TRUE y\")\n",
        "plt.xlabel(\"number of training-set objects N\")\n",
        "plt.title(\"varying N at fixed D, K, P, Q, C_x, C_y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuLzyCA_e5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make humanK plot at fiducialN, fiducialD\n",
        "\n",
        "D = fiducialD\n",
        "N = fiducialN\n",
        "humanKs = np.unique(np.round(2. ** np.arange(np.log2(K) - 1.5, np.log2(D) + 1.01, 0.25)).astype(int))\n",
        "dys_K = np.zeros((len(humanKs), 4, Ntrial, Ntest, M))\n",
        "for ii, humanK in enumerate(humanKs):\n",
        "  print(\"starting trials for humanK =\", humanK)\n",
        "  humanP = np.eye(humanK)[:M, :]\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    Bs.append(B)\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    Gs_dwh.append(G_dwh)\n",
        "    dys_K[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_K[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_K[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_K[ii, 3, trial] = ys_test - (W @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5iBWAD1N-OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_K = np.mean(np.mean(np.mean(dys_K ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.min(mses_K)\n",
        "y2 = np.max(mses_K) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(humanKs, mses_K[:, 3], y1 + 0. * mses_K[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(humanKs, mses_K[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(humanKs, mses_K[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(humanKs, mses_K[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(humanKs, mses_K[:, 3], \"k-\", lw=0.5, label=\"best possible\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of TRUE y\")\n",
        "plt.xlabel(\"size humanK of human-created latent space\")\n",
        "plt.title(\"varying humanK at fixed D, N, K, P, Q, C_y, C_x\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy1jDPz7N-dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8l0R6B-N-n1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKaRSDJN_euR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWfJ_A0_hF8",
        "colab_type": "text"
      },
      "source": [
        "# OLD STUFF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7NZJKjnUHXX",
        "colab_type": "text"
      },
      "source": [
        "# OLD STUFF FOLLOWS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBcWfPVUF7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make things to histogram\n",
        "Bs, Gs_msv, Gs_dwh = trainings # unpack\n",
        "Ntrials, M, D = Bs.shape\n",
        "Ntest = 2 ** 5 + 5\n",
        "dys_discriminative = np.zeros((Ntrials, Ntest, M))\n",
        "dys_msv = np.zeros((Ntrials, Ntest, M))\n",
        "dys_dwh = np.zeros((Ntrials, Ntest, M))\n",
        "for nn in range(Ntrials):\n",
        "  xtest, ytest = make_data_set(Ntest, P, Q, xivars, yivars) # generate with God's knowledge\n",
        "  dys_discriminative[nn] = ytest - (Bs[nn] @ xtest.T).T\n",
        "  dys_msv[nn] = ytest - (Gs_msv[nn] @ xtest.T).T\n",
        "  dys_dwh[nn] = ytest - (Gs_dwh[nn] @ xtest.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y3UKgjWUGG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(131)\n",
        "foo, bins, bar = plt.hist(dys_discriminative.reshape((Ntrials, Ntest)).T, bins=Ntest // 5, stacked=True)\n",
        "plt.title(\"discriminative prediction of y\")\n",
        "plt.subplot(132)\n",
        "plt.hist(dys_msv.reshape((Ntrials, Ntest)).T, bins=bins, stacked=True)\n",
        "plt.title(\"MSV generative prediction of y\")\n",
        "plt.subplot(133)\n",
        "plt.hist(dys_dwh.reshape((Ntrials, Ntest)).T, bins=bins, stacked=True)\n",
        "plt.title(\"DWH generative prediction of y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z54DE55nUGlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute means and variances\n",
        "means = np.mean(trainings, axis=1)\n",
        "mean2s = np.mean(trainings ** 2, axis=1)\n",
        "variances = mean2s - means ** 2\n",
        "variances = np.sum(np.sum(variances, axis=-1), axis=-1)\n",
        "print(variances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGAwQlts6mj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OLD plotting macro\n",
        "def plot_inference(labels, inference, title, name):\n",
        "  foo = np.array([-20, 20])\n",
        "  bar = 1.1 * np.max(np.abs(labels))\n",
        "  plt.figure(figsize=(15,5))\n",
        "  plt.subplot(131)\n",
        "  plt.scatter(labels, inference)\n",
        "  plt.title(title + \" inference (vs labels) for \" + name)\n",
        "  plt.xlabel(\"label slope\")\n",
        "  plt.ylabel(\"inferred slope\")\n",
        "  plt.plot(foo, foo, \"k-\", alpha=0.25)\n",
        "  plt.xlim(-bar, bar)\n",
        "  plt.ylim(-bar, bar)\n",
        "  print(\"RMSE vs labels:\", rms(labels - inference))\n",
        "  #plt.subplot(132)\n",
        "  #plt.scatter(truth, inference)\n",
        "  #plt.title(title + \" inference (vs truth) for \" + name)\n",
        "  #plt.xlabel(\"true slope\")\n",
        "  #plt.ylabel(\"inferred slope\")\n",
        "  #plt.plot(foo, foo, \"k-\", alpha=0.25)\n",
        "  #plt.xlim(-bar, bar)\n",
        "  #plt.ylim(-bar, bar)\n",
        "  #print(\"RMSE vs truth:\", rms(truth - inference))\n",
        "  plt.subplot(132)\n",
        "  plt.scatter(labels, inference - labels)\n",
        "  plt.title(title + \" inference for \" + name)\n",
        "  plt.xlabel(\"label slope\")\n",
        "  plt.ylabel(\"residual (inferred - label)\")\n",
        "  plt.plot(foo, 0. * foo, \"k-\", alpha=0.25)\n",
        "  plt.xlim(-bar, bar)\n",
        "  plt.ylim(-0.3 * bar, 0.3 * bar)\n",
        "  plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVPJwA-8GnNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot and report outcomes for the efficient estimator\n",
        "# this should be the best you can possibly do\n",
        "m = 0\n",
        "plot_inference(ys_test[:, m], ys_efficient_test[:, m],\n",
        "               \"\", \"the efficient estimator\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6-YLM-hHGUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot and report outcomes for the discriminative model\n",
        "plot_inference(ys_test[:, m], ys_discriminative_test[:, m],\n",
        "               \"\", \"the discriminative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKEspa7zoGBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_inference(ys_test[:, m], ys_msv_test[:, m],\n",
        "               \"\", \"the MSV generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKl5HgsXru8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_inference(ys_test[:, m], ys_trivial_test[:, m],\n",
        "               \"\", \"the alt-trivial generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cI3rIhIcFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_dwh = train_dwh_generative_model(xs_train, ys_train, humanP, xivars, yivars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIBdWcVTRMVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ys_dwh_test = (G_dwh @ xs_test.T).T\n",
        "plot_inference(ys_test[:, m], ys_dwh_test[:, m],\n",
        "               \"\", \"the DWH generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNFff75KBrCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,4,sharey=True, figsize=(18,4.5))\n",
        "for m in range(M):\n",
        "  ax[0].plot(W[m, :], alpha=0.75)\n",
        "ax[0].set_title(\"true (information-theory-optimal) result\")\n",
        "for m in range(M):\n",
        "  ax[1].plot(B[m, :], alpha=0.75)\n",
        "ax[1].set_title(\"discriminative result\")\n",
        "for m in range(M):\n",
        "  ax[2].plot(G_msv[m, :], alpha=0.75)\n",
        "ax[2].set_title(\"MSV generative result\")\n",
        "for m in range(M):\n",
        "  ax[3].plot(Hdagger[m, :], alpha=0.75)\n",
        "ax[3].set_title(\"alt-trivial generative result\")\n",
        "ax[3].set_ylim(np.min(W), np.max(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPnDosmwRUBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,4,sharey=True, figsize=(18,4.5))\n",
        "for m in range(M):\n",
        "  ax[0].plot(W[m, :], alpha=0.75)\n",
        "ax[0].set_title(\"true (information-theory-optimal) result\")\n",
        "for m in range(M):\n",
        "  ax[1].plot(G_msv[m, :], alpha=0.75)\n",
        "ax[1].set_title(\"MSV generative result\")\n",
        "for m in range(M):\n",
        "  ax[2].plot(G_dwh[m, :], alpha=0.75)\n",
        "ax[2].set_title(\"DWH generative result\")\n",
        "for m in range(M):\n",
        "  ax[3].plot(Hdagger[m, :], alpha=0.75)\n",
        "ax[3].set_title(\"alt-trivial generative result\")\n",
        "ax[3].set_ylim(np.min(W), np.max(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jNqVHeDVCML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_many_times(Ntrain, humanP, P, Q, xivars, yivars, Ntrials=2**5):\n",
        "  \"\"\"\n",
        "  Perform many trainings of each model for the purposes of empirically testing\n",
        "  biases and variances.\n",
        "\n",
        "  ## inputs:\n",
        "  - Ntrain - training set size N\n",
        "  - humanP - human-set projector P\n",
        "  - P - God's projector P\n",
        "  - Q - God's embeddor Q\n",
        "  - xivars - diagonal of C_x\\inv\n",
        "  - yivars - diagonal of C_y\\inv\n",
        "  - Ntrials (optional) - number of independent trials\n",
        "  \"\"\"\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrials):\n",
        "    xtrain, ytrain = make_data_set(Ntrain, P, Q, xivars, yivars) # generate with God's knowledge\n",
        "    Bs.append(train_discriminative_model(xtrain, ytrain))\n",
        "    Gs_msv.append(train_msv_generative_model(xtrain, ytrain, humanP, iters=1000)) # infer with human choices\n",
        "    Gs_dwh.append(train_dwh_generative_model(xtrain, ytrain, humanP, xivars, yivars, maxiter=1000))\n",
        "  return np.array([np.array(Bs), np.array(Gs_msv), np.array(Gs_dwh)])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}