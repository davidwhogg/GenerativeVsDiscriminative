{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative_vs_discriminative.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/GenerativeVsDiscriminative/blob/master/ipynb/generative_vs_discriminative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpggye9TxS-s",
        "colab_type": "text"
      },
      "source": [
        "# Generative *vs* Discriminative models for inference\n",
        "\n",
        "- Do generative models generally outperform discriminative models in various ways?\n",
        "- How do both of these compare to information-theory-optimal estimators?\n",
        "- Are generative models less subject to adversarial attack?\n",
        "- Are generative and discriminative models equally good for de-noising noisy labels?\n",
        "- ... and related matters.\n",
        "\n",
        "## authors\n",
        "- **David W Hogg** *(NYU) (MPIA) (Flatiron)*\n",
        "- **Soledad Villar** *(NYU)*\n",
        "\n",
        "## license\n",
        "Copyright 2019, 2020 the authors. All rights reserved (for now).\n",
        "\n",
        "## notes and to-do items\n",
        "- Consider making a new kind of method that fits a Gaussian to the joint of `X` and `Y`.\n",
        "- Rearrange things so we can loop over some amplitudes of `C_x` and `C_y` as well.\n",
        "- Decide what are our main results / observations.\n",
        "- Fix colors in plots so objects of similar ontology are in similar colors and styles?\n",
        "- It's cool that he noise contributions are mean zero, but I (DWH) don't think the latent pdf for `Z` should be mean zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLXl4Vx8WYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "#matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYQYdl_g8t-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the data-generating matrices of God\n",
        "# BUG: This should probably be a function call.\n",
        "\n",
        "# set God's integers\n",
        "maximalD = 2 ** 9 # ESLII calls this \"p\"\n",
        "fiducialD = 2 ** 7\n",
        "M = 1 # ESLII always sets this to 1\n",
        "K = 2 ** 4\n",
        "assert(M <= K)\n",
        "\n",
        "# set training set size\n",
        "maximalN = maximalD\n",
        "fiducialN = 2 ** 6\n",
        "Ntest = 2 ** 8 # number of points to use to compute biases and variances\n",
        "Ntrial = 2 ** 4 # number of training trials to use to compute biases and variances\n",
        "\n",
        "# set God's matrices\n",
        "# the true world can be a (K-1)-order Legendre (orthogonal) or Chebyshev (non-orthogonal) polynomial\n",
        "#Q = np.vstack([np.polynomial.legendre.legval(np.arange(-1. + 1./D, 1., 2./D),\n",
        "#                                             np.eye(K)[k])\n",
        "#    for k in range(K)]).T\n",
        "# or the true world can be a set of random matrices\n",
        "np.random.seed(42)\n",
        "maximalQ = np.random.normal(size=(maximalD, K))\n",
        "np.random.seed(17)\n",
        "P = np.random.normal(size=(M, K))\n",
        "\n",
        "# make noise covariance matrices\n",
        "np.random.seed(13)\n",
        "foo = np.random.normal(size=(maximalD, 2 * maximalD))\n",
        "maximalCx = 2 ** -2 * foo @ foo.T\n",
        "foo = np.random.normal(size=(M, 2 * M))\n",
        "Cy = 2 ** -2 * foo @ foo.T\n",
        "foo = np.random.normal(size=(K, 2 * K))\n",
        "V = foo @ foo.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uci_lRUKkatw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gods_estimator(P, Q, V, Cx):\n",
        "  \"\"\"\n",
        "  make the best possible estimator, in terms of mean squared error on y\n",
        "  (this, we hope, is what everything approaches, in the limit N -> infinity)\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Q @ V @ Q.T + Cx, Q @ V @ P.T,\n",
        "                         rcond=None)[0].T\n",
        "\n",
        "def gods_unbiased_estimator(P, Q, Cx):\n",
        "  \"\"\"\n",
        "  make the minimum-variance estimator of y given x, subject to the requirement\n",
        "  that it be unbiased for y, conditioned on x.\n",
        "  \"\"\"\n",
        "  D, K = Q.shape\n",
        "  M, KK = P.shape\n",
        "  assert K == KK, \"gods_unbiased_estimator(): P, Q shapes wrong\"\n",
        "  if D < K:\n",
        "    return np.nan + np.zeros((M, D))\n",
        "  wQ = np.linalg.lstsq(Cx, Q, rcond=None)[0]\n",
        "  return P @ np.linalg.lstsq(wQ.T @ Q, wQ.T, rcond=None)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7z7OH--1MSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maximalW = gods_estimator(P, maximalQ, V, maximalCx)\n",
        "maximalW.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUGYRRYKlFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data_set(N, P, Q, V, Cx, Cy):\n",
        "  \"\"\"\n",
        "  actually make the random data, using the matrices of God\n",
        "  \"\"\"\n",
        "  M, K = P.shape\n",
        "  D, KK = Q.shape\n",
        "  assert K == KK\n",
        "  zs = np.random.multivariate_normal(np.zeros(K), V, size=N)\n",
        "  godxs = (Q @ zs.T).T # \"true\" xs\n",
        "  xs = godxs + np.random.multivariate_normal(np.zeros(D), Cx, size=N)\n",
        "  godys = (P @ zs.T).T # \"true\" ys\n",
        "  ys = godys + np.random.multivariate_normal(np.zeros(M), Cy, size=N)\n",
        "  return xs, ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggWSWinWMD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make maximal train and test sets\n",
        "np.random.seed(8675309 + 1)\n",
        "maximalX = np.zeros((Ntrial, maximalN, maximalD))\n",
        "maximalY = np.zeros((Ntrial, maximalN, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalX[trial], maximalY[trial] = make_data_set(maximalN, P, maximalQ, V, maximalCx, Cy)\n",
        "maximalXtest = np.zeros((Ntrial, Ntest, maximalD))\n",
        "maximalYtest = np.zeros((Ntrial, Ntest, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalXtest[trial], maximalYtest[trial] = make_data_set(Ntest, P, maximalQ, V, maximalCx, Cy)\n",
        "print(maximalX.shape, maximalY.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI_hm117kftA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u, s, v = np.linalg.svd(maximalX[0, :fiducialN, :fiducialD], full_matrices=False)\n",
        "plt.axvline(K)\n",
        "plt.plot(s, \"ko\")\n",
        "plt.semilogy()\n",
        "plt.ylabel(\"contribution to variance\")\n",
        "plt.xlabel(\"component number\")\n",
        "plt.title(\"SVD of the fiducial data set, zeroth trial\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGrRc7kO8-8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#examples = np.arange(4)\n",
        "#colors=['red', 'purple', 'green', 'blue']\n",
        "#for i,n in enumerate(examples):\n",
        "#  plt.step(np.arange(maximalD), xs_train[n], linestyle='-',alpha=0.5, color=colors[i], where=\"mid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjsOAJNShEdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminative_model(xs, ys, lamb=0.):\n",
        "  \"\"\"\n",
        "  train discriminative model y = B x + noise\n",
        "  \n",
        "  ## inputs:\n",
        "  - xs - array of training data\n",
        "  - ys - array of training labels\n",
        "\n",
        "  ## comments\n",
        "  Informally speaking, this finds the B that minimizes || ys - B . xs || (plus\n",
        "  some regularization), and returns the discriminative matrix B.\n",
        "\n",
        "  ## bugs:\n",
        "  - Properly, this should take in an estimate of the inverse variances. These\n",
        "    might be some processing of the xivars and yivars; we need to figure that\n",
        "    out.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  N, D = xs.shape\n",
        "  NN, M = ys.shape\n",
        "  assert N == NN\n",
        "  return np.linalg.lstsq(xs.T @ xs + lamb * np.eye(D), xs.T @ ys, rcond=None)[0].T\n",
        "\n",
        "def train_trivial_generative_model(xs, ys):\n",
        "  \"\"\"\n",
        "  say stuff here\n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(ys, xs, rcond=None)[0]\n",
        "\n",
        "def initialize_a_generative_model(Y, P):\n",
        "  Z = np.linalg.lstsq(P, Y.T, rcond=None)[0]\n",
        "  proj_P = P.T @ np.linalg.lstsq(P @ P.T, P, rcond=None)[0]\n",
        "  M, humanK = P.shape\n",
        "  proj_Pperp = np.identity(humanK) - proj_P\n",
        "  return proj_P @ Z, proj_Pperp @ np.random.normal(size=Z.shape), proj_Pperp\n",
        "\n",
        "def train_msv_generative_model(X, Y, P, maxiter=10000, Z=None):\n",
        "  \"\"\"\n",
        "  ## Inputs:\n",
        "  X: N x D\n",
        "  Y: N x M\n",
        "  P: M x humanK\n",
        "\n",
        "  ## Comments:\n",
        "  among all solutions Z such that Y=Z*P.T (ie Y.T=P*Z.T)\n",
        "  we minimize ||X - Z*A|| by alternating minimization\n",
        "  we write Z = Z0 + Z1, proj_P*Z=Z0, proj_Pperp*Z = Z1 \n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't use xivars, yivars.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  M, humanK = P.shape\n",
        "  \n",
        "  Z0, Z1, proj_Pperp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = Z0 + Z1\n",
        "  tiny = 1.e-9\n",
        "  unconverged = True\n",
        "  last_cost=np.inf\n",
        "  for i in range(maxiter):\n",
        "    # normalize the perpendicular part of Z\n",
        "    Z1 = proj_Pperp @ Z\n",
        "    Z -= Z1\n",
        "    Z1 /= tiny + np.sqrt(np.mean(Z1 ** 2, axis=1))[:, None]\n",
        "    Z += Z1\n",
        "\n",
        "    #fix Z optimize for A\n",
        "    A=np.linalg.lstsq(Z.T, X, rcond=None)[0]\n",
        "\n",
        "    #fix A optimize for Z\n",
        "    a = A.T @ proj_Pperp.T\n",
        "    b = X.T - A.T @ Z0\n",
        "    Z1 = np.linalg.lstsq(a, b, rcond=None)[0]\n",
        "    Z1 = proj_Pperp @ Z1\n",
        "\n",
        "    #convergence\n",
        "    Z_old = Z\n",
        "    Z = Z0 + Z1\n",
        "    #BUG: unstable! \n",
        "    #print('convergence, cost')\n",
        "    cost= np.linalg.norm(X.T - A.T @ Z)\n",
        "\n",
        "    if cost >= last_cost:\n",
        "      unconverged = False\n",
        "      break\n",
        "    last_cost = cost\n",
        "    #print((np.linalg.norm(Z-Z_old), np.linalg.norm(X.T- np.matmul(A.T,Z)) + np.linalg.norm(Y.T -np.matmul(P,Z)) ))\n",
        "  if unconverged: print(\"train_msv_generative_model(): WARNING: did not converge.\")\n",
        "  return  P @ np.linalg.lstsq(A @ A.T, A, rcond=None)[0], Z.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rgWZgx0Kjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  bigA = np.vstack((sqrtCxinv @ A, sqrtCyinv @ P))\n",
        "  bigX = np.hstack((X @ sqrtCxinv, Y @ sqrtCyinv))\n",
        "  return np.linalg.lstsq(bigA, bigX.T, rcond=None)[0].T\n",
        "\n",
        "def a_step(X, Z):\n",
        "  \"\"\"\n",
        "  BUG: This doesn't currently use the Cx, Cy correctly.\n",
        "  BUG: meaning: It treats the data as homoskedastic.\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Z, X, rcond=None)[0].T\n",
        "\n",
        "def dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Untested!\n",
        "  \"\"\"\n",
        "  xchi = sqrtCxinv @ (X.T - (A @ Z.T))\n",
        "  ychi = sqrtCyinv @ (Y.T - (P @ Z.T))\n",
        "  return np.sum(xchi ** 2) + np.sum(ychi ** 2)\n",
        "\n",
        "def train_dwh_generative_model(X, Y, P, Cx, Cy, maxiter=10000, lltol=0.00001, Z=None):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Initialization bad here. This should probably initialize at the internal\n",
        "    state of the MSV generative model optimized earlier.\n",
        "  - Takes forever to converge when K = M, apparently. This may be related to\n",
        "    initialization also.\n",
        "  \"\"\"\n",
        "  # get read the square roots of the inverses of Cx, Cy\n",
        "  u, s, v = np.linalg.svd(Cx)\n",
        "  sqrtCxinv = (u / np.sqrt(s)) @ v\n",
        "  Cxinv = (u / s) @ v\n",
        "  u, s, v = np.linalg.svd(Cy)\n",
        "  sqrtCyinv = (u / np.sqrt(s)) @ v\n",
        "\n",
        "  # initialize loop\n",
        "  Z0, Z1, proj_P_perp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = (Z0 + Z1).T\n",
        "  tiny = 1.e-9\n",
        "  cost = np.inf\n",
        "  unconverged = True\n",
        "  A = 0.\n",
        "\n",
        "  # loop\n",
        "  for ii in range(maxiter):\n",
        "    old_cost = 1. * cost\n",
        "    oldA = 1. * A\n",
        "\n",
        "    # normalization step\n",
        "    Zperp = (proj_P_perp @ Z.T).T\n",
        "    Z -= Zperp\n",
        "    Zperp /= tiny + np.sqrt(np.mean(Zperp ** 2, axis=0))[None, :]\n",
        "    Z += Zperp\n",
        "\n",
        "    # standard iteration\n",
        "    A = a_step(X, Z)\n",
        "    Z = z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv)\n",
        "\n",
        "    # convergence control\n",
        "    cost = dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv)\n",
        "    if cost > 1. and cost > (old_cost + lltol): # note insane condition\n",
        "      print(\"train_dwh_generative_model(): WARNING: cost went the wrong way!\")\n",
        "      print(ii, cost, old_cost)\n",
        "      A = oldA # restore\n",
        "    if cost > (old_cost - lltol):\n",
        "      unconverged = False\n",
        "      break\n",
        "  if unconverged: print(\"train_dwh_generative_model(): WARNING: did not converge.\")\n",
        "  wA = np.linalg.lstsq(Cx, A, rcond=None)[0]\n",
        "  return P @ np.linalg.lstsq(wA.T @ A, wA.T, rcond=None)[0], Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNiam-vuF-vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run on the first trial case, for the fiducials\n",
        "xs_train = maximalX[0, :fiducialN, :fiducialD]\n",
        "ys_train = maximalY[0, :fiducialN, :]\n",
        "xs_test = maximalXtest[0, :, :fiducialD]\n",
        "ys_test = maximalYtest[0, :, :]\n",
        "Cx = maximalCx[:fiducialD, :fiducialD]\n",
        "W = gods_estimator(P, maximalQ[:fiducialD, :], V, Cx)\n",
        "\n",
        "print(xs_train.shape, ys_train.shape)\n",
        "B = train_discriminative_model(xs_train, ys_train)\n",
        "Hdagger = train_trivial_generative_model(xs_train, ys_train)\n",
        "print(B.shape, Hdagger.shape)\n",
        "\n",
        "# humanP = P\n",
        "# humanP = np.hstack((P, np.zeros((M, 2))))\n",
        "# humanP = P[:, :9]\n",
        "humanK = K # stupid\n",
        "assert humanK >= M\n",
        "humanP = np.eye(humanK)[:M, :]\n",
        "print(P.shape, humanP.shape)\n",
        "G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "# hack: Use MSV's latents to initialize DWH's model\n",
        "G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "\n",
        "ys_efficient_test = (W @ xs_test.T).T #W is information theory optimal\n",
        "ys_msv_test = (G_msv @ xs_test.T).T\n",
        "ys_dwh_test = (G_dwh @ xs_test.T).T\n",
        "ys_discriminative_test = (B @ xs_test.T).T\n",
        "ys_trivial_test = (Hdagger @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpSfFNVabyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rms(x):\n",
        "  return np.sqrt(np.mean(x * x))\n",
        "\n",
        "m = 0\n",
        "print(\"best possible\", rms(ys_test[:, m] - ys_efficient_test[:, m]))\n",
        "print(\"discriminative\", rms(ys_test[:, m] - ys_discriminative_test[:, m]))\n",
        "print(\"alt-trivial generative\", rms(ys_test[:, m] - ys_trivial_test[:, m]))\n",
        "print(\"MSV generative\", rms(ys_test[:, m] - ys_msv_test[:, m]))\n",
        "print(\"DWH generative\", rms(ys_test[:, m] - ys_dwh_test[:, m]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYR1db7byM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(151)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_discriminative_test[:, m], bins=32)\n",
        "plt.title(\"discriminative\")\n",
        "plt.subplot(155)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_efficient_test[:, m], bins=bins)\n",
        "plt.title(\"God's best\")\n",
        "plt.subplot(152)\n",
        "plt.hist(ys_test[:, m] - ys_trivial_test[:, m], bins=bins)\n",
        "plt.title(\"alt-trivial generative\")\n",
        "plt.subplot(153)\n",
        "plt.hist(ys_test[:, m] - ys_msv_test[:, m], bins=bins)\n",
        "plt.title(\"MSV generative\")\n",
        "plt.subplot(154)\n",
        "plt.hist(ys_test[:, m] - ys_dwh_test[:, m], bins=bins)\n",
        "plt.title(\"DWH generative\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6b_5uRdsye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mean_variance(ixs):\n",
        "  xs = np.array(ixs)\n",
        "  mean = np.mean(xs)\n",
        "  var = np.mean(xs ** 2) - mean ** 2\n",
        "  return mean, var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjZub21_eoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make D plot at fiducialN\n",
        "lamb = 0.\n",
        "humanK = K # stupid\n",
        "N = fiducialN\n",
        "Ds = np.round((maximalD * 2 ** np.arange(-6.5, 0.01, 0.25))).astype(int)\n",
        "dys_D = np.zeros((len(Ds), 5, Ntrial, Ntest, M))\n",
        "Ws, Bs, Gs_msv, Gs_dwh = [], [], [], []\n",
        "for ii, D in enumerate(Ds):\n",
        "  print(\"starting trials for D =\", D)\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  Ws.append(W)\n",
        "  W2 = gods_unbiased_estimator(P, maximalQ[:D, :], Cx)\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train, lamb=lamb)\n",
        "    if trial == 0:\n",
        "      Bs.append(B)\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    if trial == 0:\n",
        "      Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    if trial == 0:\n",
        "      Gs_dwh.append(G_dwh)\n",
        "    dys_D[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_D[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_D[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_D[ii, 3, trial] = ys_test - (W @ xs_test.T).T\n",
        "    dys_D[ii, 4, trial] = ys_test - (W2 @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcBpKpa_exu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_D = np.mean(np.mean(np.mean(dys_D ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_D)\n",
        "y2 = np.nanmax(mses_D) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ds, mses_D[:, 3], y1 + 0. * mses_D[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ds, mses_D[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ds, mses_D[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ds, mses_D[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(Ds, mses_D[:, 4], \"k-\", lw=0.5, alpha=0.5, label=\"best unbiased\")\n",
        "plt.plot(Ds, mses_D[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K = humanK\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"number of image pixels D\")\n",
        "plt.title(\"varying D at fixed N, K, P, C_y; truncating or extending Q, C_x\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH_-5JZpcFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make N plot at fiducialD\n",
        "\n",
        "humanK = K # stupid\n",
        "D = fiducialD\n",
        "Ns = np.round((maximalN * 2 ** np.arange(-6., 0.01, 0.25))).astype(int)\n",
        "dys_N = np.zeros((len(Ns), 5, Ntrial, Ntest, M))\n",
        "for ii, N in enumerate(Ns):\n",
        "  print(\"starting trials for N =\", N)\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  W2 = gods_unbiased_estimator(P, maximalQ[:D, :], Cx)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train, lamb=lamb)\n",
        "    Bs.append(B)\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    Gs_dwh.append(G_dwh)\n",
        "    # re-run as a test\n",
        "    # G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP, Z=zs.T)\n",
        "    dys_N[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_N[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_N[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_N[ii, 3, trial] = ys_test - (W @ xs_test.T).T\n",
        "    dys_N[ii, 4, trial] = ys_test - (W2 @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSz9OKd1Yjq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_N = np.mean(np.mean(np.mean(dys_N ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_N)\n",
        "y2 = np.nanmax(mses_N) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ns, mses_N[:, 3], y1 + 0. * mses_N[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ns, mses_N[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ns, mses_N[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ns, mses_N[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(Ns, mses_N[:, 4], \"k-\", lw=0.5, alpha=0.5, label=\"best unbiased\")\n",
        "plt.plot(Ns, mses_N[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K = humanK\", color=\"r\", alpha=0.5)\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"number of training-set objects N\")\n",
        "plt.title(\"varying N at fixed D, K, P, Q, C_x, C_y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuLzyCA_e5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make humanK plot at fiducialN, fiducialD\n",
        "\n",
        "D = fiducialD\n",
        "N = fiducialN\n",
        "humanKs = np.unique(np.round(2. ** np.arange(np.log2(K) - 1.5, np.log2(D) + 0.76, 0.25)).astype(int))\n",
        "dys_K = np.zeros((len(humanKs), 5, Ntrial, Ntest, M))\n",
        "for ii, humanK in enumerate(humanKs):\n",
        "  print(\"starting trials for humanK =\", humanK)\n",
        "  humanP = np.eye(humanK)[:M, :]\n",
        "  Cx = maximalCx[:D, :D]\n",
        "  W = gods_estimator(P, maximalQ[:D, :], V, Cx)\n",
        "  W2 = gods_unbiased_estimator(P, maximalQ[:D, :], Cx)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :D]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, :D]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train, lamb=lamb)\n",
        "    Bs.append(B)\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, humanP)\n",
        "    Gs_msv.append(G_msv)\n",
        "    G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, humanP, Cx, Cy, Z=zs)\n",
        "    Gs_dwh.append(G_dwh)\n",
        "    dys_K[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_K[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_K[ii, 2, trial] = ys_test - (G_dwh @ xs_test.T).T\n",
        "    dys_K[ii, 3, trial] = ys_test - (W @ xs_test.T).T\n",
        "    dys_K[ii, 4, trial] = ys_test - (W2 @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5iBWAD1N-OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_K = np.mean(np.mean(np.mean(dys_K ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_K)\n",
        "y2 = np.nanmax(mses_K) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(humanKs, mses_K[:, 3], y1 + 0. * mses_K[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(humanKs, mses_K[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(humanKs, mses_K[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(humanKs, mses_K[:, 2], \"go\", mfc=\"none\", ms=10, label=\"DWH generative\")\n",
        "plt.plot(humanKs, mses_K[:, 4], \"k-\", lw=0.5, alpha=0.5, label=\"best unbiased\")\n",
        "plt.plot(humanKs, mses_K[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"size humanK of human-created latent space\")\n",
        "plt.title(\"varying humanK at fixed D, N, K, P, Q, C_y, C_x\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy1jDPz7N-dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}