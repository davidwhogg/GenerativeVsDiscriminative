{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative_vs_discriminative.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/GenerativeVsDiscriminative/blob/master/ipynb/generative_vs_discriminative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpggye9TxS-s",
        "colab_type": "text"
      },
      "source": [
        "# Generative *vs* Discriminative models for inference\n",
        "\n",
        "- Do generative models generally outperform discriminative models in various ways?\n",
        "- How do both of these compare to information-theory-optimal estimators?\n",
        "- Are generative models less subject to adversarial attack?\n",
        "- Are generative and discriminative models equally good for de-noising noisy labels?\n",
        "- ... and related matters.\n",
        "\n",
        "## authors\n",
        "- **David W Hogg** *(NYU) (MPIA) (Flatiron)*\n",
        "- **Soledad Villar** *(NYU)*\n",
        "\n",
        "## license\n",
        "Copyright 2019, 2020 the authors. All rights reserved (for now).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLXl4Vx8WYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "#matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYQYdl_g8t-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the data-generating matrices of God\n",
        "# BUG: This should probably be a function call.\n",
        "\n",
        "# set God's integers\n",
        "maximalD = 2 ** 10 # ESLII calls this \"p\"\n",
        "fiducialD = 2 ** 8\n",
        "M = 1 # ESLII always sets this to 1\n",
        "K = 42\n",
        "assert(M <= K)\n",
        "\n",
        "# set training set size\n",
        "maximalN = maximalD\n",
        "fiducialN = 2 ** 8\n",
        "Ntest = 2 ** 12 # number of points to use to compute biases and variances\n",
        "Ntrial = 2 ** 4 # number of training trials to use to compute biases and variances\n",
        "\n",
        "# set God's matrices\n",
        "# the true world can be a (K-1)-order Legendre (orthogonal) or Chebyshev (non-orthogonal) polynomial\n",
        "#Q = np.vstack([np.polynomial.legendre.legval(np.arange(-1. + 1./D, 1., 2./D),\n",
        "#                                             np.eye(K)[k])\n",
        "#    for k in range(K)]).T\n",
        "# or the true world can be a set of random matrices\n",
        "np.random.seed(42)\n",
        "maximalQ = np.random.normal(size=(maximalD, K))\n",
        "np.random.seed(17)\n",
        "P = np.random.normal(size=(M, K))\n",
        "\n",
        "# We aren't being very general about the noise\n",
        "maximalxivars = np.zeros(maximalD) + 2. ** 4 # inverse of diagonal of C_x ; all C_xn identical and proportional to I\n",
        "yivars = np.zeros(M) + 2. ** 4 # inverse of diagonal of C_y; same for C_yn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE9CzcvwkjdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define functions to take and use pseudo-inverses.\n",
        "\n",
        "def get_lambda(M, maxcond, lam):\n",
        "  \"\"\"\n",
        "  internal function for pseudo_inverse() and pseudo_solve()\n",
        "  \"\"\"\n",
        "  if lam is not None:\n",
        "    assert maxcond is None, \"get_lambda(): You can't set both maxcond and lam\"\n",
        "    return lam\n",
        "  if maxcond is None:\n",
        "    return 0.\n",
        "  assert maxcond > 1., \"get_lambda(): maxcond must be > 1\"\n",
        "  return np.max(np.linalg.eigvalsh(M)) / maxcond\n",
        "\n",
        "def pseudo_solve(A, x, Cinv=None, weights=None, maxcond=None, lam=None):\n",
        "  \"\"\"\n",
        "  return solve(ATA, ATx) but be somewhat clever about numerics\n",
        "\n",
        "  ## inputs:\n",
        "  - A: rectangular matrix\n",
        "  - Cinv (optional): square non-negative semi-definite matrix that can multiply\n",
        "      AT on the right or A on the left.\n",
        "  - weight (optional): diagonal of Cinv if Cinv is diagonal (conflicts with\n",
        "      Cinv).\n",
        "  - maxcond (optional): a regularization parameter >> 1; costs a eigvalsh()\n",
        "      call.\n",
        "  - lam (optional): a regularization parameter (conflicts with maxcond).\n",
        "\n",
        "  ## outputs:\n",
        "  - Pseudo-inverse of A applied to x.\n",
        "  \"\"\"\n",
        "  foo, bar = A.shape # need this to decide whether to do ATA or AAT\n",
        "\n",
        "  # use weights or Cinv to weight A.T\n",
        "  if weights is not None:\n",
        "    assert Cinv is None, \"pseudo_inverse(): You can't set both Cinv and weight\"\n",
        "    wAT = weights * A.T\n",
        "  elif Cinv is not None:\n",
        "    wAT = A.T @ Cinv\n",
        "  else:\n",
        "    wAT = A.T\n",
        "\n",
        "  if foo < bar:\n",
        "    AwAT = A @ wAT\n",
        "    lamb = get_lambda(AwAT, maxcond, lam)\n",
        "    if x is None:\n",
        "      return np.linalg.solve(AwAT + lamb * np.eye(foo), wAT.T).T\n",
        "    return wAT @ np.linalg.solve(AwAT + lamb * np.eye(foo), x) # There are many ways to write this line\n",
        "\n",
        "  if x is None:\n",
        "    wATx = wAT\n",
        "  else:\n",
        "    wATx = wAT @ x\n",
        "\n",
        "  wATA = wAT @ A\n",
        "  lamb = get_lambda(wATA, maxcond, lam)\n",
        "  return np.linalg.solve(wATA + lamb * np.eye(bar), wATx)\n",
        "\n",
        "def pseudo_inverse(A, Cinv=None, weights=None, maxcond=None, lam=None):\n",
        "  \"\"\"\n",
        "  return solve(ATA, AT) but be somewhat clever about numerics\n",
        "  \"\"\"\n",
        "  return pseudo_solve(A, None, Cinv=Cinv, weights=weights, maxcond=maxcond, lam=lam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uci_lRUKkatw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gods_estimator(P, Q, xivars):\n",
        "  \"\"\"\n",
        "  make the best possible estimator\n",
        "  (this, we hope, is what everything approaches, in the limit)\n",
        "  note super-safe linear algebra stupidity\n",
        "  \"\"\"\n",
        "  return P @ pseudo_inverse(Q, weights=xivars, maxcond=1e9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7z7OH--1MSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maximalW = gods_estimator(P, maximalQ, maximalxivars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUGYRRYKlFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data_set(N, P, Q, xivars, yivars):\n",
        "  \"\"\"\n",
        "  actually make the random data, using the matrices of God\n",
        "  \"\"\"\n",
        "  M, K = P.shape\n",
        "  D, KK = Q.shape\n",
        "  assert K == KK\n",
        "  zs = np.random.normal(size=(N, K))\n",
        "  godxs = (Q @ zs.T).T # \"true\" xs\n",
        "  xs = godxs + np.random.normal(size=(N, D)) / np.sqrt(xivars)\n",
        "  godys = (P @ zs.T).T # \"true\" ys\n",
        "  ys = godys + np.random.normal(size=(N, M)) / np.sqrt(yivars)\n",
        "  return xs, ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggWSWinWMD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make maximal train and test sets\n",
        "# NOTE THAT THE TEST Y HAS INFINITE yivars OR ZERO NOISE\n",
        "np.random.seed(8675309 + 1)\n",
        "maximalX = np.zeros((Ntrial, maximalN, maximalD))\n",
        "maximalY = np.zeros((Ntrial, maximalN, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalX[trial], maximalY[trial] = make_data_set(maximalN, P, maximalQ, maximalxivars, yivars)\n",
        "maximalXtest, maximalYtest = make_data_set(Ntest, P, maximalQ, maximalxivars, np.inf * yivars)\n",
        "print(maximalX.shape, maximalY.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI_hm117kftA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#u, s, v = np.linalg.svd(maximalX, full_matrices=False)\n",
        "#plt.plot(s, \"ko\")\n",
        "#plt.semilogy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGrRc7kO8-8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#examples = np.arange(4)\n",
        "#colors=['red', 'purple', 'green', 'blue']\n",
        "#for i,n in enumerate(examples):\n",
        "#  plt.step(np.arange(maximalD), xs_train[n], linestyle='-',alpha=0.5, color=colors[i], where=\"mid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjsOAJNShEdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminative_model(xs, ys, maxcond=1.e9):\n",
        "  \"\"\"\n",
        "  train discriminative model y = B x + noise\n",
        "  \n",
        "  ## inputs:\n",
        "  - xs - array of training data\n",
        "  - ys - array of training labels\n",
        "  - maxcond (optional) - limit on the condition number of the matrix inversions\n",
        "\n",
        "  ## comments\n",
        "  Informally speaking, this finds the B that minimizes || ys - B . xs || (plus\n",
        "  some regularization), and returns the discriminative matrix B.\n",
        "\n",
        "  ## bugs:\n",
        "  - Properly, this should take in an estimate of the inverse variances. These\n",
        "    might be some processing of the xivars and yivars; we need to figure that\n",
        "    out.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return pseudo_solve(xs, ys, maxcond=maxcond).T\n",
        "\n",
        "def train_trivial_generative_model(xs, ys, maxcond=1.e9):\n",
        "  \"\"\"\n",
        "  say stuff here\n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return pseudo_solve(ys, xs, maxcond=maxcond)\n",
        "\n",
        "def initialize_a_generative_model(Y, P):\n",
        "  \"\"\"\n",
        "  HOGG say: This can be simplified with pseudo_inverse().\n",
        "  \"\"\"\n",
        "  Z = np.linalg.lstsq(P, Y.T, rcond=None)[0]\n",
        "  P1 = P.T\n",
        "  proj_P = np.matmul(P1, np.linalg.solve(np.matmul(P1.T, P1), P1.T))\n",
        "  M, humanK = P.shape\n",
        "  proj_Pperp = np.identity(humanK) - proj_P\n",
        "  return np.matmul(proj_P, Z), np.matmul(proj_Pperp, np.random.random(Z.shape))\n",
        "\n",
        "def train_msv_generative_model(X, Y, P, iters=1000):\n",
        "  \"\"\"\n",
        "  ## Inputs:\n",
        "  X: N x D\n",
        "  Y: N x M\n",
        "  P: M x humanK\n",
        "\n",
        "  ## Comments:\n",
        "  among all solutions Z such that Y=Z*P.T (ie Y.T=P*Z.T)\n",
        "  we minimize ||X - Z*A|| by alternating minimization\n",
        "  we write Z = Z0 + Z1, proj_P*Z=Z0, proj_Pperp*Z = Z1 \n",
        "\n",
        "  ## bugs:\n",
        "  - Can be simplified with pseudo_inverse().\n",
        "  - Doesn't use xivars, yivars.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  proj_P= np.matmul(P.T, np.linalg.solve(np.matmul(P, P.T), P))\n",
        "  M, humanK = P.shape\n",
        "  proj_Pperp = np.identity(humanK) - proj_P\n",
        "  \n",
        "  Z0, Z1 = initialize_a_generative_model(Y, P)\n",
        "  Z = Z0 + Z1\n",
        "  last_cost=np.inf\n",
        "  for i in range(iters):  \n",
        "    #fix Z optimize for A\n",
        "    A=np.linalg.lstsq(Z.T, X, rcond=None)[0]\n",
        "\n",
        "    #fix A optimize for Z\n",
        "    a = np.matmul(A.T, proj_Pperp.T)\n",
        "    b = X.T- np.matmul(A.T, Z0)\n",
        "    Z1=np.linalg.lstsq(a,b, rcond=None)[0]\n",
        "\n",
        "    Z1=np.matmul(proj_Pperp, Z1)\n",
        "\n",
        "    #convergence\n",
        "    Z_old=Z\n",
        "    Z=Z0+Z1\n",
        "    #BUG: unstable! \n",
        "    #print('convergence, cost')\n",
        "    cost= np.linalg.norm(X.T- np.matmul(A.T,Z)) \n",
        "\n",
        "    if cost >= last_cost:\n",
        "      break\n",
        "    last_cost=cost\n",
        "    #print((np.linalg.norm(Z-Z_old), np.linalg.norm(X.T- np.matmul(A.T,Z)) + np.linalg.norm(Y.T -np.matmul(P,Z)) ))\n",
        "  return  P @ np.linalg.solve(A @ A.T, A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rgWZgx0Kjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def z_step(X, Y, A, P, xivars, yivars):\n",
        "  bigA = np.vstack((A, P))\n",
        "  bigivars = np.append(xivars, yivars)\n",
        "  bigX = np.hstack((X, Y))\n",
        "  return pseudo_solve(bigA, bigX.T, weights=bigivars).T\n",
        "\n",
        "def a_step(X, Z, xivars):\n",
        "  \"\"\"\n",
        "  BUG: This doesn't currently use the ivars correctly.\n",
        "  BUG: meaning: It treats the data as homoskedastic.\n",
        "  \"\"\"\n",
        "  return pseudo_solve(Z, X).T\n",
        "\n",
        "def dwh_cost(X, Y, Z, A, P, xivars, yivars):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Untested!\n",
        "  \"\"\"\n",
        "  xresid = X - (A @ Z.T).T\n",
        "  yresid = Y - (P @ Z.T).T\n",
        "  return np.sum(xresid ** 2 * xivars) + np.sum(yresid ** 2 * yivars)\n",
        "\n",
        "def train_dwh_generative_model(X, Y, P, xivars, yivars, maxiter=3, lltol=0.0001):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Initialization bad here. This should probably initialize at the internal\n",
        "    state of the MSV generative model optimized earlier.\n",
        "  - Takes forever to converge when K = M, apparently. This may be related to\n",
        "    initialization also.\n",
        "  - TOTAL HACK TO DEAL WITH infinite xivars.\n",
        "  \"\"\"\n",
        "  Z0, Z1 = initialize_a_generative_model(Y, P)\n",
        "  Z = (Z0 + Z1).T\n",
        "  cost = np.inf\n",
        "  unconverged = True\n",
        "  A = 0.\n",
        "  for ii in range(maxiter):\n",
        "    old_cost = 1. * cost\n",
        "    oldA = 1. * A\n",
        "    A = a_step(X, Z, xivars)\n",
        "    Z = z_step(X, Y, A, P, xivars, yivars)\n",
        "    cost = dwh_cost(X, Y, Z, A, P, xivars, yivars)\n",
        "    if cost > (old_cost + lltol):\n",
        "      print(\"train_dwh_generative_model(): WARNING: cost went the wrong way!\")\n",
        "      print(ii, cost, old_cost)\n",
        "      A = oldA # restore\n",
        "    if cost > (old_cost - lltol):\n",
        "      unconverged = False\n",
        "      break\n",
        "  if unconverged: print(\"train_dwh_generative_model(): WARNING: did not converge.\")\n",
        "  return P @ pseudo_inverse(A, weights=xivars, maxcond=1e9) # this maxcond is required for some reason"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNiam-vuF-vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run on the first trial case, for the fiducials\n",
        "xs_train = maximalX[0, :fiducialN, :fiducialD]\n",
        "ys_train = maximalY[0, :fiducialN, :]\n",
        "xs_test = maximalXtest[:, :fiducialD]\n",
        "ys_test = maximalYtest\n",
        "xivars = maximalxivars[:fiducialD]\n",
        "W = gods_estimator(P, maximalQ[:fiducialD, :], xivars)\n",
        "\n",
        "print(xs_train.shape, ys_train.shape)\n",
        "B = train_discriminative_model(xs_train, ys_train)\n",
        "Hdagger = train_trivial_generative_model(xs_train, ys_train)\n",
        "print(B.shape, Hdagger.shape)\n",
        "\n",
        "# humanP = P\n",
        "# humanP = np.hstack((P, np.zeros((M, 2))))\n",
        "# humanP = P[:, :9]\n",
        "humanK = K # stupid\n",
        "assert humanK >= M\n",
        "humanP = np.eye(humanK)[:M, :]\n",
        "print(P.shape, humanP.shape)\n",
        "G_msv = train_msv_generative_model(xs_train, ys_train, humanP, iters=1000)\n",
        "G_dwh = train_dwh_generative_model(xs_train, ys_train, humanP, xivars, yivars, maxiter=1000)\n",
        "\n",
        "ys_efficient_test = (W @ xs_test.T).T #W is information theory optimal\n",
        "ys_msv_test = (G_msv @ xs_test.T).T\n",
        "ys_dwh_test = (G_dwh @ xs_test.T).T\n",
        "ys_discriminative_test = (B @ xs_test.T).T\n",
        "ys_trivial_test = (Hdagger @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpSfFNVabyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rms(x):\n",
        "  return np.sqrt(np.mean(x * x))\n",
        "\n",
        "m = 0\n",
        "print(\"best possible\", rms(ys_test[:, m] - ys_efficient_test[:, m]))\n",
        "print(\"discriminative\", rms(ys_test[:, m] - ys_discriminative_test[:, m]))\n",
        "print(\"alt-trivial generative\", rms(ys_test[:, m] - ys_trivial_test[:, m]))\n",
        "print(\"MSV generative\", rms(ys_test[:, m] - ys_msv_test[:, m]))\n",
        "print(\"DWH generative\", rms(ys_test[:, m] - ys_dwh_test[:, m]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYR1db7byM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(151)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_discriminative_test[:, m], bins=32)\n",
        "plt.title(\"discriminative\")\n",
        "plt.subplot(155)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_efficient_test[:, m], bins=bins)\n",
        "plt.title(\"God's best\")\n",
        "plt.subplot(152)\n",
        "plt.hist(ys_test[:, m] - ys_trivial_test[:, m], bins=bins)\n",
        "plt.title(\"alt-trivial generative\")\n",
        "plt.subplot(153)\n",
        "plt.hist(ys_test[:, m] - ys_msv_test[:, m], bins=bins)\n",
        "plt.title(\"MSV generative\")\n",
        "plt.subplot(154)\n",
        "plt.hist(ys_test[:, m] - ys_dwh_test[:, m], bins=bins)\n",
        "plt.title(\"DWH generative\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6b_5uRdsye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mean_variance(ixs):\n",
        "  xs = np.array(ixs)\n",
        "  mean = np.mean(xs)\n",
        "  var = np.mean(xs ** 2) - mean ** 2\n",
        "  return mean, var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH_-5JZpcFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make N plot at fiducialD\n",
        "\n",
        "humanK = K # stupid\n",
        "Ns = np.round((maximalN / 2 ** np.arange(0., 6.51, 0.25))[::-1]).astype(int)\n",
        "dys_N = np.zeros((len(Ns), 4, Ntrial, Ntest, M))\n",
        "for ii, N in enumerate(Ns):\n",
        "  print(\"starting trials for N =\", N)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, :fiducialD]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[:, :fiducialD]\n",
        "    ys_test = maximalYtest\n",
        "    xivars = maximalxivars[:fiducialD]\n",
        "    W = gods_estimator(P, maximalQ[:fiducialD, :], xivars)\n",
        "    Bs.append(train_discriminative_model(xs_train, ys_train))\n",
        "    assert humanK >= M\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    Gs_msv.append(train_msv_generative_model(xs_train, ys_train, humanP, iters=1000))\n",
        "    Gs_dwh.append(train_dwh_generative_model(xs_train, ys_train, humanP, xivars, yivars, maxiter=1000))\n",
        "  dys_N[ii, 0] = np.array([ys_test - (B @ xs_test.T).T for B in Bs])\n",
        "  dys_N[ii, 1] = np.array([ys_test - (G @ xs_test.T).T for G in Gs_msv])\n",
        "  dys_N[ii, 2] = np.array([ys_test - (G @ xs_test.T).T for G in Gs_dwh])\n",
        "  dys_N[ii, 3] = np.array([ys_test - (W @ xs_test.T).T for B in Bs]) # note stupid repeat of the same shit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSz9OKd1Yjq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_N = np.mean(np.mean(np.mean(dys_N ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "plt.plot(Ns, mses_N[:, 3], \"k-\")\n",
        "plt.plot(Ns, mses_N[:, 0], \"ko\")\n",
        "plt.plot(Ns, mses_N[:, 1], \"ro\")\n",
        "plt.plot(Ns, mses_N[:, 2], \"go\")\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.axvline(humanK, color=\"g\", alpha=0.5)\n",
        "plt.loglog()\n",
        "plt.ylabel(\"MSE in prediction of TRUE y (variance + bias^2)\")\n",
        "plt.xlabel(\"number of training set objects N\")\n",
        "plt.title(\"varying N at fixed D\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjZub21_eoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make D plot at fiducialN\n",
        "\n",
        "humanK = K # stupid\n",
        "Ds = np.round((maximalD / 2 ** np.arange(0., 6.51, 0.25))[::-1]).astype(int)\n",
        "dys_D = np.zeros((len(Ds), 4, Ntrial, Ntest, M))\n",
        "for ii, D in enumerate(Ds):\n",
        "  print(\"starting trials for D =\", D)\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :fiducialN, :D]\n",
        "    ys_train = maximalY[trial, :fiducialN, :]\n",
        "    xs_test = maximalXtest[:, :D]\n",
        "    ys_test = maximalYtest\n",
        "    xivars = maximalxivars[:D]\n",
        "    W = gods_estimator(P, maximalQ[:D, :], xivars)\n",
        "    Bs.append(train_discriminative_model(xs_train, ys_train))\n",
        "    assert humanK >= M\n",
        "    humanP = np.eye(humanK)[:M, :]\n",
        "    Gs_msv.append(train_msv_generative_model(xs_train, ys_train, humanP, iters=1000))\n",
        "    Gs_dwh.append(train_dwh_generative_model(xs_train, ys_train, humanP, xivars, yivars, maxiter=1000))\n",
        "  dys_D[ii, 0] = np.array([ys_test - (B @ xs_test.T).T for B in Bs])\n",
        "  dys_D[ii, 1] = np.array([ys_test - (G @ xs_test.T).T for G in Gs_msv])\n",
        "  dys_D[ii, 2] = np.array([ys_test - (G @ xs_test.T).T for G in Gs_dwh])\n",
        "  dys_D[ii, 3] = np.array([ys_test - (W @ xs_test.T).T for B in Bs]) # note stupid repeat of the same shit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcBpKpa_exu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_D = np.mean(np.mean(np.mean(dys_D ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "plt.plot(Ds, mses_D[:, 3], \"k-\")\n",
        "plt.plot(Ds, mses_D[:, 0], \"ko\")\n",
        "plt.plot(Ds, mses_D[:, 1], \"ro\")\n",
        "plt.plot(Ds, mses_D[:, 2], \"go\")\n",
        "plt.axvline(fiducialN, color=\"k\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.axvline(humanK, color=\"g\", alpha=0.5)\n",
        "plt.loglog()\n",
        "plt.ylabel(\"MSE in prediction of TRUE y (variance + bias^2)\")\n",
        "plt.xlabel(\"number of image pixels D\")\n",
        "plt.title(\"varying D at fixed N\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuLzyCA_e5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKaRSDJN_euR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWfJ_A0_hF8",
        "colab_type": "text"
      },
      "source": [
        "# OLD STUFF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7NZJKjnUHXX",
        "colab_type": "text"
      },
      "source": [
        "# OLD STUFF FOLLOWS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBcWfPVUF7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make things to histogram\n",
        "Bs, Gs_msv, Gs_dwh = trainings # unpack\n",
        "Ntrials, M, D = Bs.shape\n",
        "Ntest = 2 ** 5 + 5\n",
        "dys_discriminative = np.zeros((Ntrials, Ntest, M))\n",
        "dys_msv = np.zeros((Ntrials, Ntest, M))\n",
        "dys_dwh = np.zeros((Ntrials, Ntest, M))\n",
        "for nn in range(Ntrials):\n",
        "  xtest, ytest = make_data_set(Ntest, P, Q, xivars, yivars) # generate with God's knowledge\n",
        "  dys_discriminative[nn] = ytest - (Bs[nn] @ xtest.T).T\n",
        "  dys_msv[nn] = ytest - (Gs_msv[nn] @ xtest.T).T\n",
        "  dys_dwh[nn] = ytest - (Gs_dwh[nn] @ xtest.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y3UKgjWUGG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(131)\n",
        "foo, bins, bar = plt.hist(dys_discriminative.reshape((Ntrials, Ntest)).T, bins=Ntest // 5, stacked=True)\n",
        "plt.title(\"discriminative prediction of y\")\n",
        "plt.subplot(132)\n",
        "plt.hist(dys_msv.reshape((Ntrials, Ntest)).T, bins=bins, stacked=True)\n",
        "plt.title(\"MSV generative prediction of y\")\n",
        "plt.subplot(133)\n",
        "plt.hist(dys_dwh.reshape((Ntrials, Ntest)).T, bins=bins, stacked=True)\n",
        "plt.title(\"DWH generative prediction of y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z54DE55nUGlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute means and variances\n",
        "means = np.mean(trainings, axis=1)\n",
        "mean2s = np.mean(trainings ** 2, axis=1)\n",
        "variances = mean2s - means ** 2\n",
        "variances = np.sum(np.sum(variances, axis=-1), axis=-1)\n",
        "print(variances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGAwQlts6mj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OLD plotting macro\n",
        "def plot_inference(labels, inference, title, name):\n",
        "  foo = np.array([-20, 20])\n",
        "  bar = 1.1 * np.max(np.abs(labels))\n",
        "  plt.figure(figsize=(15,5))\n",
        "  plt.subplot(131)\n",
        "  plt.scatter(labels, inference)\n",
        "  plt.title(title + \" inference (vs labels) for \" + name)\n",
        "  plt.xlabel(\"label slope\")\n",
        "  plt.ylabel(\"inferred slope\")\n",
        "  plt.plot(foo, foo, \"k-\", alpha=0.25)\n",
        "  plt.xlim(-bar, bar)\n",
        "  plt.ylim(-bar, bar)\n",
        "  print(\"RMSE vs labels:\", rms(labels - inference))\n",
        "  #plt.subplot(132)\n",
        "  #plt.scatter(truth, inference)\n",
        "  #plt.title(title + \" inference (vs truth) for \" + name)\n",
        "  #plt.xlabel(\"true slope\")\n",
        "  #plt.ylabel(\"inferred slope\")\n",
        "  #plt.plot(foo, foo, \"k-\", alpha=0.25)\n",
        "  #plt.xlim(-bar, bar)\n",
        "  #plt.ylim(-bar, bar)\n",
        "  #print(\"RMSE vs truth:\", rms(truth - inference))\n",
        "  plt.subplot(132)\n",
        "  plt.scatter(labels, inference - labels)\n",
        "  plt.title(title + \" inference for \" + name)\n",
        "  plt.xlabel(\"label slope\")\n",
        "  plt.ylabel(\"residual (inferred - label)\")\n",
        "  plt.plot(foo, 0. * foo, \"k-\", alpha=0.25)\n",
        "  plt.xlim(-bar, bar)\n",
        "  plt.ylim(-0.3 * bar, 0.3 * bar)\n",
        "  plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVPJwA-8GnNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot and report outcomes for the efficient estimator\n",
        "# this should be the best you can possibly do\n",
        "m = 0\n",
        "plot_inference(ys_test[:, m], ys_efficient_test[:, m],\n",
        "               \"\", \"the efficient estimator\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6-YLM-hHGUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot and report outcomes for the discriminative model\n",
        "plot_inference(ys_test[:, m], ys_discriminative_test[:, m],\n",
        "               \"\", \"the discriminative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKEspa7zoGBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_inference(ys_test[:, m], ys_msv_test[:, m],\n",
        "               \"\", \"the MSV generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKl5HgsXru8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_inference(ys_test[:, m], ys_trivial_test[:, m],\n",
        "               \"\", \"the alt-trivial generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cI3rIhIcFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_dwh = train_dwh_generative_model(xs_train, ys_train, humanP, xivars, yivars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIBdWcVTRMVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ys_dwh_test = (G_dwh @ xs_test.T).T\n",
        "plot_inference(ys_test[:, m], ys_dwh_test[:, m],\n",
        "               \"\", \"the DWH generative model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNFff75KBrCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,4,sharey=True, figsize=(18,4.5))\n",
        "for m in range(M):\n",
        "  ax[0].plot(W[m, :], alpha=0.75)\n",
        "ax[0].set_title(\"true (information-theory-optimal) result\")\n",
        "for m in range(M):\n",
        "  ax[1].plot(B[m, :], alpha=0.75)\n",
        "ax[1].set_title(\"discriminative result\")\n",
        "for m in range(M):\n",
        "  ax[2].plot(G_msv[m, :], alpha=0.75)\n",
        "ax[2].set_title(\"MSV generative result\")\n",
        "for m in range(M):\n",
        "  ax[3].plot(Hdagger[m, :], alpha=0.75)\n",
        "ax[3].set_title(\"alt-trivial generative result\")\n",
        "ax[3].set_ylim(np.min(W), np.max(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPnDosmwRUBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,4,sharey=True, figsize=(18,4.5))\n",
        "for m in range(M):\n",
        "  ax[0].plot(W[m, :], alpha=0.75)\n",
        "ax[0].set_title(\"true (information-theory-optimal) result\")\n",
        "for m in range(M):\n",
        "  ax[1].plot(G_msv[m, :], alpha=0.75)\n",
        "ax[1].set_title(\"MSV generative result\")\n",
        "for m in range(M):\n",
        "  ax[2].plot(G_dwh[m, :], alpha=0.75)\n",
        "ax[2].set_title(\"DWH generative result\")\n",
        "for m in range(M):\n",
        "  ax[3].plot(Hdagger[m, :], alpha=0.75)\n",
        "ax[3].set_title(\"alt-trivial generative result\")\n",
        "ax[3].set_ylim(np.min(W), np.max(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jNqVHeDVCML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_many_times(Ntrain, humanP, P, Q, xivars, yivars, Ntrials=2**5):\n",
        "  \"\"\"\n",
        "  Perform many trainings of each model for the purposes of empirically testing\n",
        "  biases and variances.\n",
        "\n",
        "  ## inputs:\n",
        "  - Ntrain - training set size N\n",
        "  - humanP - human-set projector P\n",
        "  - P - God's projector P\n",
        "  - Q - God's embeddor Q\n",
        "  - xivars - diagonal of C_x\\inv\n",
        "  - yivars - diagonal of C_y\\inv\n",
        "  - Ntrials (optional) - number of independent trials\n",
        "  \"\"\"\n",
        "  Bs, Gs_msv, Gs_dwh = [], [], []\n",
        "  for trial in range(Ntrials):\n",
        "    xtrain, ytrain = make_data_set(Ntrain, P, Q, xivars, yivars) # generate with God's knowledge\n",
        "    Bs.append(train_discriminative_model(xtrain, ytrain))\n",
        "    Gs_msv.append(train_msv_generative_model(xtrain, ytrain, humanP, iters=1000)) # infer with human choices\n",
        "    Gs_dwh.append(train_dwh_generative_model(xtrain, ytrain, humanP, xivars, yivars, maxiter=1000))\n",
        "  return np.array([np.array(Bs), np.array(Gs_msv), np.array(Gs_dwh)])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}