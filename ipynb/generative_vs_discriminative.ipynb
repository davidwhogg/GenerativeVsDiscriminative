{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative_vs_discriminative.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/GenerativeVsDiscriminative/blob/master/ipynb/generative_vs_discriminative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpggye9TxS-s",
        "colab_type": "text"
      },
      "source": [
        "# Generative *vs* Discriminative models for inference\n",
        "\n",
        "- Do generative models generally outperform discriminative models in various ways?\n",
        "- How do both of these compare to information-theory-optimal estimators?\n",
        "- Are generative models less subject to adversarial attack?\n",
        "- Are generative and discriminative models equally good for de-noising noisy labels?\n",
        "- ... and related matters.\n",
        "\n",
        "## authors\n",
        "- **David W Hogg** *(NYU) (MPIA) (Flatiron)*\n",
        "- **Soledad Villar** *(NYU)*\n",
        "\n",
        "## license\n",
        "Copyright 2019, 2020 the authors. All rights reserved (for now).\n",
        "\n",
        "## notes and to-do items\n",
        "- The latent space Gaussian should have a non-zero mean and the discriminative model needs `np.ones()` appended to the `X`.\n",
        "- Decide what are our main results / observations.\n",
        "- Fix colors in plots so objects of similar ontology are in similar colors and styles?\n",
        "- It's cool that he noise contributions are mean zero, but I (DWH) don't think the latent pdf for `Z` should be mean zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLXl4Vx8WYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYQYdl_g8t-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set integers\n",
        "\n",
        "# set data dimensionality\n",
        "maximalD = 2 ** 9 # ESLII calls this \"p\"\n",
        "fiducialD = 2 ** 7\n",
        "M = 1 # ESLII always sets this to 1\n",
        "\n",
        "# set training and test set sizes\n",
        "fiducialN = 2 ** 6\n",
        "maximalN = maximalD # maximum possible size of a training set\n",
        "Ntest = 2 ** 8 # number of points to use to compute biases and variances\n",
        "Ntrial = 2 ** 4 # number of training trials to use to compute biases and variances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Ij--mUAfUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set God's X-Y covariance matrix (M+D x M+D)\n",
        "\n",
        "# start with a random matrix\n",
        "np.random.seed(13)\n",
        "foo = np.random.normal(size=(M + maximalD, M + maximalD))\n",
        "bar = foo @ foo.T\n",
        "\n",
        "# and then manipulate eigenvalues\n",
        "u, ss, v = np.linalg.svd(bar)\n",
        "# s = 1.e5 * np.exp(-3.00 * np.arange(M + maximalD)) \\\n",
        "#   + 0.01 * np.exp(-0.01 * np.arange(M + maximalD))\n",
        "s = 1. * ss\n",
        "s[:8] = 1.e4 * s[:8]\n",
        "maximalC = (u * s) @ v\n",
        "\n",
        "\"\"\"\n",
        "GodK = 16\n",
        "Q = np.random.normal(size=(maximalD, GodK))\n",
        "P = np.random.normal(size=(M, GodK))\n",
        "foo = np.random.normal(size=(GodK, GodK + 4))\n",
        "V = foo @ foo.T\n",
        "foo = np.random.normal(size=(maximalD, 2 * maximalD))\n",
        "Cx = 2 ** -4 * foo @ foo.T\n",
        "foo = np.random.normal(size=(M, 2 * M))\n",
        "Cy = 2 ** -4 * foo @ foo.T\n",
        "maximalC = np.vstack((np.hstack((Q @ V @ Q.T + Cx, Q @ V @ P.T)),\n",
        "                      np.hstack((P @ V @ Q.T, P @ V @ P.T + Cy))))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUGYRRYKlFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data_set(N, C, M):\n",
        "  \"\"\"\n",
        "  actually make the random data, using the variance of God.\n",
        "\n",
        "  Needs to be updated to take God's mean.\n",
        "  \"\"\"\n",
        "  MplusD, foo = C.shape\n",
        "  D = MplusD - M\n",
        "  xys = np.random.multivariate_normal(np.zeros(MplusD), C, size=N)\n",
        "  return xys[:, :D], xys[:, D:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggWSWinWMD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make maximal train and test sets\n",
        "np.random.seed(8675309 + 1)\n",
        "maximalX = np.zeros((Ntrial, maximalN, maximalD))\n",
        "maximalY = np.zeros((Ntrial, maximalN, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalX[trial], maximalY[trial] = make_data_set(maximalN, maximalC, M)\n",
        "maximalXtest = np.zeros((Ntrial, Ntest, maximalD))\n",
        "maximalYtest = np.zeros((Ntrial, Ntest, M))\n",
        "for trial in range(Ntrial):\n",
        "  maximalXtest[trial], maximalYtest[trial] = make_data_set(Ntest, maximalC, M)\n",
        "print(maximalX.shape, maximalY.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI_hm117kftA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u, s, v = np.linalg.svd(maximalX[0, :fiducialN, -fiducialD:], full_matrices=False)\n",
        "plt.plot(s ** 2, \"ko\")\n",
        "C = maximalC[-fiducialD-M:, -fiducialD-M:]\n",
        "uu, ss, vv = np.linalg.svd(C)\n",
        "plt.plot((fiducialN * ss)[:fiducialN], \"ko\", alpha=0.24)\n",
        "print(np.sum(ss), np.trace(C))\n",
        "plt.semilogy()\n",
        "plt.ylabel(\"contribution to variance\")\n",
        "plt.xlabel(\"component number\")\n",
        "plt.title(\"SVD of the fiducial data set, zeroth trial\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uci_lRUKkatw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gods_estimator(C, M):\n",
        "  \"\"\"\n",
        "  WARNING: This function needs to know about God's mean mu, and\n",
        "  be fixed to incorporate it.\n",
        "\n",
        "  make the best possible estimator, in terms of mean squared error on y\n",
        "  (this, we hope, is what everything approaches, in the limit N -> infinity)\n",
        "  \"\"\"\n",
        "  MplusD, foo = C.shape\n",
        "  D = MplusD - M\n",
        "  Cxx = C[:D, :D]\n",
        "  Cxy = C[:D, D:]\n",
        "  return np.linalg.lstsq(Cxx, Cxy, rcond=None)[0].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjsOAJNShEdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminative_model(xs, ys, lamb=0.):\n",
        "  \"\"\"\n",
        "  train discriminative model y = B x + noise\n",
        "  \n",
        "  ## inputs:\n",
        "  - xs - array of training data\n",
        "  - ys - array of training labels\n",
        "\n",
        "  ## comments\n",
        "  Informally speaking, this finds the B that minimizes || ys - B . xs || (plus\n",
        "  some regularization), and returns the discriminative matrix B.\n",
        "\n",
        "  ## bugs:\n",
        "  - Properly, this should take in an estimate of the inverse variances. These\n",
        "    might be some processing of the xivars and yivars; we need to figure that\n",
        "    out.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  N, D = xs.shape\n",
        "  NN, M = ys.shape\n",
        "  assert N == NN\n",
        "  return np.linalg.lstsq(xs.T @ xs + lamb * np.eye(D), xs.T @ ys, rcond=None)[0].T\n",
        "\n",
        "def train_trivial_generative_model(xs, ys):\n",
        "  \"\"\"\n",
        "  say stuff here\n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(ys, xs, rcond=None)[0]\n",
        "\n",
        "def initialize_a_generative_model(Y, P):\n",
        "  Z = np.linalg.lstsq(P, Y.T, rcond=None)[0]\n",
        "  proj_P = P.T @ np.linalg.lstsq(P @ P.T, P, rcond=None)[0]\n",
        "  M, K = P.shape\n",
        "  proj_Pperp = np.identity(K) - proj_P\n",
        "  return proj_P @ Z, proj_Pperp @ np.random.normal(size=Z.shape), proj_Pperp\n",
        "\n",
        "def train_msv_generative_model(X, Y, P, maxiter=10000, Z=None):\n",
        "  \"\"\"\n",
        "  ## Inputs:\n",
        "  X: N x D\n",
        "  Y: N x M\n",
        "  P: M x K\n",
        "\n",
        "  ## Comments:\n",
        "  among all solutions Z such that Y=Z*P.T (ie Y.T=P*Z.T)\n",
        "  we minimize ||X - Z*A|| by alternating minimization\n",
        "  we write Z = Z0 + Z1, proj_P*Z=Z0, proj_Pperp*Z = Z1 \n",
        "\n",
        "  ## bugs:\n",
        "  - Doesn't use xivars, yivars.\n",
        "  - Doesn't fit for a bias term (that is, a zero-offset).\n",
        "  \"\"\"\n",
        "  M, K = P.shape\n",
        "  \n",
        "  Z0, Z1, proj_Pperp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = Z0 + Z1\n",
        "  tiny = 1.e-9\n",
        "  unconverged = True\n",
        "  last_cost=np.inf\n",
        "  for i in range(maxiter):\n",
        "    # normalize the perpendicular part of Z\n",
        "    Z1 = proj_Pperp @ Z\n",
        "    Z -= Z1\n",
        "    Z1 /= tiny + np.sqrt(np.mean(Z1 ** 2, axis=1))[:, None]\n",
        "    Z += Z1\n",
        "\n",
        "    #fix Z optimize for A\n",
        "    A=np.linalg.lstsq(Z.T, X, rcond=None)[0]\n",
        "\n",
        "    #fix A optimize for Z\n",
        "    a = A.T @ proj_Pperp.T\n",
        "    b = X.T - A.T @ Z0\n",
        "    Z1 = np.linalg.lstsq(a, b, rcond=None)[0]\n",
        "    Z1 = proj_Pperp @ Z1\n",
        "\n",
        "    #convergence\n",
        "    Z_old = Z\n",
        "    Z = Z0 + Z1\n",
        "    #BUG: unstable! \n",
        "    #print('convergence, cost')\n",
        "    cost= np.linalg.norm(X.T - A.T @ Z)\n",
        "\n",
        "    if cost >= last_cost:\n",
        "      unconverged = False\n",
        "      break\n",
        "    last_cost = cost\n",
        "    #print((np.linalg.norm(Z-Z_old), np.linalg.norm(X.T- np.matmul(A.T,Z)) + np.linalg.norm(Y.T -np.matmul(P,Z)) ))\n",
        "  if unconverged: print(\"train_msv_generative_model(): WARNING: did not converge.\")\n",
        "  return  P @ np.linalg.lstsq(A @ A.T, A, rcond=None)[0], Z.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rgWZgx0Kjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  bigA = np.vstack((sqrtCxinv @ A, sqrtCyinv @ P))\n",
        "  bigX = np.hstack((X @ sqrtCxinv, Y @ sqrtCyinv))\n",
        "  return np.linalg.lstsq(bigA, bigX.T, rcond=None)[0].T\n",
        "\n",
        "def a_step(X, Z):\n",
        "  \"\"\"\n",
        "  BUG: This doesn't currently use the Cx, Cy correctly.\n",
        "  BUG: meaning: It treats the data as homoskedastic.\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Z, X, rcond=None)[0].T\n",
        "\n",
        "def dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Untested!\n",
        "  \"\"\"\n",
        "  xchi = sqrtCxinv @ (X.T - (A @ Z.T))\n",
        "  ychi = sqrtCyinv @ (Y.T - (P @ Z.T))\n",
        "  return np.sum(xchi ** 2) + np.sum(ychi ** 2)\n",
        "\n",
        "def train_dwh_generative_model(X, Y, P, Cx, Cy, maxiter=10000, lltol=0.00001, Z=None):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Initialization bad here. This should probably initialize at the internal\n",
        "    state of the MSV generative model optimized earlier.\n",
        "  - Takes forever to converge when K = M, apparently. This may be related to\n",
        "    initialization also.\n",
        "  \"\"\"\n",
        "  # get read the square roots of the inverses of Cx, Cy\n",
        "  u, s, v = np.linalg.svd(Cx)\n",
        "  sqrtCxinv = (u / np.sqrt(s)) @ v\n",
        "  Cxinv = (u / s) @ v\n",
        "  u, s, v = np.linalg.svd(Cy)\n",
        "  sqrtCyinv = (u / np.sqrt(s)) @ v\n",
        "\n",
        "  # initialize loop\n",
        "  Z0, Z1, proj_P_perp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = (Z0 + Z1).T\n",
        "  tiny = 1.e-9\n",
        "  cost = np.inf\n",
        "  unconverged = True\n",
        "  A = 0.\n",
        "\n",
        "  # loop\n",
        "  for ii in range(maxiter):\n",
        "    old_cost = 1. * cost\n",
        "    oldA = 1. * A\n",
        "\n",
        "    # normalization step\n",
        "    Zperp = (proj_P_perp @ Z.T).T\n",
        "    Z -= Zperp\n",
        "    Zperp /= tiny + np.sqrt(np.mean(Zperp ** 2, axis=0))[None, :]\n",
        "    Z += Zperp\n",
        "\n",
        "    # standard iteration\n",
        "    A = a_step(X, Z)\n",
        "    Z = z_step(X, Y, A, P, sqrtCxinv, sqrtCyinv)\n",
        "\n",
        "    # convergence control\n",
        "    cost = dwh_cost(X, Y, Z, A, P, sqrtCxinv, sqrtCyinv)\n",
        "    if cost > 1. and cost > (old_cost + lltol): # note insane condition\n",
        "      print(\"train_dwh_generative_model(): WARNING: cost went the wrong way!\")\n",
        "      print(ii, cost, old_cost)\n",
        "      A = oldA # restore\n",
        "    if cost > (old_cost - lltol):\n",
        "      unconverged = False\n",
        "      break\n",
        "  if unconverged: print(\"train_dwh_generative_model(): WARNING: did not converge.\")\n",
        "  wA = np.linalg.lstsq(Cx, A, rcond=None)[0]\n",
        "  return P @ np.linalg.lstsq(wA.T @ A, wA.T, rcond=None)[0], Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_E0nZPgENO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pca_generative_model(X, Y, K):\n",
        "  \"\"\"\n",
        "  DOESN'T DEAL WITH MEAN X CORRECTLY.\n",
        "  \"\"\"\n",
        "  u, s, v = np.linalg.svd(X, full_matrices=False)\n",
        "  B = train_discriminative_model((u * s)[:, :K], Y)\n",
        "  return B @ v[:K, :]\n",
        "\n",
        "def wow_a_step(X, Z):\n",
        "  \"\"\"\n",
        "  internal function for `train_wow_generative_model()`.\n",
        "  \"\"\"\n",
        "  return np.linalg.lstsq(Z, X, rcond=None)[0].T\n",
        "\n",
        "def wow_z_step(X, Y, P, wyoverwx, A):\n",
        "  \"\"\"\n",
        "  internal function for `train_wow_generative_model()`.\n",
        "  \"\"\"\n",
        "  wx, wy = 1. / np.sqrt(wyoverwx), np.sqrt(wyoverwx)\n",
        "  bigA = np.vstack((A * np.sqrt(wx), P * np.sqrt(wy)))\n",
        "  bigX = np.hstack((X * np.sqrt(wx), Y * np.sqrt(wy)))\n",
        "  return np.linalg.lstsq(bigA, bigX.T, rcond=None)[0].T\n",
        "\n",
        "def train_wow_generative_model(X, Y, P, wyoverwx, maxiter=10000, Z=None):\n",
        "  \"\"\"\n",
        "  ## Bugs:\n",
        "  - Initialization bad here. This should probably initialize at the internal\n",
        "    state of the MSV generative model optimized earlier.\n",
        "  - Also it converges very very slowly when wyoverwx is small.\n",
        "  \"\"\"\n",
        "  # initialize loop\n",
        "  Z0, Z1, proj_P_perp = initialize_a_generative_model(Y, P)\n",
        "  if Z is None:\n",
        "    Z = (Z0 + Z1).T\n",
        "  tiny = 1.e-9\n",
        "  cost = np.inf\n",
        "  unconverged = True\n",
        "  A = 0.\n",
        "\n",
        "  # loop\n",
        "  for ii in range(maxiter):\n",
        "    old_cost = 1. * cost\n",
        "    oldA = 1. * A\n",
        "\n",
        "    # normalization step\n",
        "    Zperp = (proj_P_perp @ Z.T).T\n",
        "    Z -= Zperp\n",
        "    Zperp /= tiny + np.sqrt(np.mean(Zperp ** 2, axis=0))[None, :]\n",
        "    Z += Zperp\n",
        "\n",
        "    # standard iteration\n",
        "    A = wow_a_step(X, Z)\n",
        "    Z = wow_z_step(X, Y, P, wyoverwx, A)\n",
        "\n",
        "    # convergence control\n",
        "    if np.allclose(A, oldA):\n",
        "      unconverged = False\n",
        "      break\n",
        "  if unconverged: print(\"train_wow_generative_model(): WARNING: did not converge.\")\n",
        "  return P @ np.linalg.lstsq(A.T @ A, A.T, rcond=None)[0], Z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNiam-vuF-vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run on the first trial case, for the fiducials\n",
        "xs_train = maximalX[0, :fiducialN, -fiducialD:]\n",
        "ys_train = maximalY[0, :fiducialN, :]\n",
        "xs_test = maximalXtest[0, :, -fiducialD:]\n",
        "ys_test = maximalYtest[0, :, :]\n",
        "C = maximalC[-fiducialD-M:, -fiducialD-M:]\n",
        "W = gods_estimator(C, M)\n",
        "\n",
        "print(xs_train.shape, ys_train.shape)\n",
        "B = train_discriminative_model(xs_train, ys_train)\n",
        "Hdagger = train_trivial_generative_model(xs_train, ys_train)\n",
        "print(B.shape, Hdagger.shape)\n",
        "\n",
        "K = 16 # stupid\n",
        "assert K >= M\n",
        "P = np.eye(K)[:M, :]\n",
        "G_msv, zs = train_msv_generative_model(xs_train, ys_train, P)\n",
        "G_pca = train_pca_generative_model(xs_train, ys_train, K)\n",
        "G_wow, zs = train_wow_generative_model(xs_train, ys_train, P, 1000., Z = zs)\n",
        "# hack: Use MSV's latents to initialize DWH's model\n",
        "# G_dwh, zs = train_dwh_generative_model(xs_train, ys_train, P, Cx, Cy, Z=zs)\n",
        "\n",
        "ys_efficient_test = (W @ xs_test.T).T #W is information theory optimal\n",
        "ys_msv_test = (G_msv @ xs_test.T).T\n",
        "ys_pca_test = (G_pca @ xs_test.T).T\n",
        "ys_wow_test = (G_wow @ xs_test.T).T\n",
        "ys_discriminative_test = (B @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpSfFNVabyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rms(x):\n",
        "  return np.sqrt(np.mean(x * x))\n",
        "\n",
        "m = 0\n",
        "print(\"best possible\", rms(ys_test[:, m] - ys_efficient_test[:, m]))\n",
        "print(\"discriminative\", rms(ys_test[:, m] - ys_discriminative_test[:, m]))\n",
        "print(\"MSV generative\", rms(ys_test[:, m] - ys_msv_test[:, m]))\n",
        "print(\"PCA generative\", rms(ys_test[:, m] - ys_pca_test[:, m]))\n",
        "print(\"WOW generative\", rms(ys_test[:, m] - ys_wow_test[:, m]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYR1db7byM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(151)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_discriminative_test[:, m], bins=32)\n",
        "plt.title(\"discriminative\")\n",
        "plt.subplot(155)\n",
        "foo, bins, bar = plt.hist(ys_test[:, m] - ys_efficient_test[:, m], bins=bins)\n",
        "plt.title(\"God's best\")\n",
        "plt.subplot(152)\n",
        "plt.hist(ys_test[:, m] - ys_msv_test[:, m], bins=bins)\n",
        "plt.title(\"MSV generative\")\n",
        "plt.subplot(153)\n",
        "plt.hist(ys_test[:, m] - ys_wow_test[:, m], bins=bins)\n",
        "plt.title(\"WOW generative\")\n",
        "plt.subplot(154)\n",
        "plt.hist(ys_test[:, m] - ys_pca_test[:, m], bins=bins)\n",
        "plt.title(\"PCA generative\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuLzyCA_e5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make K plot at fiducialN, fiducialD\n",
        "D = fiducialD\n",
        "N = fiducialN\n",
        "Ks = np.unique(np.round(2. ** np.arange(0., np.log2(D) + 0.76, 0.25)).astype(int))\n",
        "dys_K = np.zeros((len(Ks), 5, Ntrial, Ntest, M)) + np.nan\n",
        "for ii, K in enumerate(Ks):\n",
        "  print(\"starting trials for K =\", K)\n",
        "  P = np.eye(K)[:M, :]\n",
        "  C = maximalC[-D-M:, -D-M:]\n",
        "  W = gods_estimator(C, M)\n",
        "  Bs, Gs_msv, Gs_pca = [], [], []\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, -D:]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, -D:]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    Bs.append(B)\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, P)\n",
        "    Gs_msv.append(G_msv)\n",
        "    G_pca = train_pca_generative_model(xs_train, ys_train, K)\n",
        "    Gs_pca.append(G_msv)\n",
        "    dys_K[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_K[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_K[ii, 2, trial] = ys_test - (G_pca @ xs_test.T).T\n",
        "    dys_K[ii, 3, trial] = ys_test - (W @ xs_test.T).T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5iBWAD1N-OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_K = np.mean(np.mean(np.mean(dys_K ** 2, axis=-1), axis=-1), axis=-1)\n",
        "bestK = Ks[np.argmin(np.nanmin(mses_K[:, 1:3], axis=-1))]\n",
        "K = bestK\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_K)\n",
        "y2 = np.nanmax(mses_K) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ks, mses_K[:, 3], y1 + 0. * mses_K[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ks, mses_K[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ks, mses_K[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ks, mses_K[:, 2], \"go\", mfc=\"none\", ms=10, label=\"PCA generative\")\n",
        "plt.plot(Ks, mses_K[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"size K of human-created latent space\")\n",
        "plt.title(\"varying K at fixed D, N, C\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjZub21_eoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make D plot at fiducialN\n",
        "K = bestK\n",
        "N = fiducialN\n",
        "Ds = np.unique(np.round(2 ** np.arange(0., np.log2(maximalD) + 0.01, 0.25)).astype(int))\n",
        "dys_D = np.zeros((len(Ds), 6, Ntrial, Ntest, M)) + np.nan\n",
        "Ws, Bs, Gs_msv, Gs_pca = [], [], [], []\n",
        "for ii, D in enumerate(Ds):\n",
        "  print(\"starting trials for D =\", D)\n",
        "  C = maximalC[-D-M:, -D-M:]\n",
        "  W = gods_estimator(C, M)\n",
        "  Ws.append(W)\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, -D:]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, -D:]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    if trial == 0:\n",
        "      Bs.append(B)\n",
        "    P = np.eye(K)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, P)\n",
        "    if trial == 0:\n",
        "      Gs_msv.append(G_msv)\n",
        "    G_pca = train_pca_generative_model(xs_train, ys_train, K)\n",
        "    if trial == 0:\n",
        "      Gs_pca.append(G_pca)\n",
        "    dys_D[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_D[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_D[ii, 2, trial] = ys_test - (G_pca @ xs_test.T).T\n",
        "    dys_D[ii, 3, trial] = ys_test - (W @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcBpKpa_exu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_D = np.mean(np.mean(np.mean(dys_D ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_D)\n",
        "y2 = np.nanmax(mses_D) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ds, mses_D[:, 3], y1 + 0. * mses_D[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ds, mses_D[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ds, mses_D[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ds, mses_D[:, 2], \"go\", mfc=\"none\", ms=10, label=\"PCA generative\")\n",
        "plt.plot(Ds, mses_D[:, 3], \"k-\", lw=0.5, label=\"God's best MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialN, color=\"b\", alpha=0.5)\n",
        "plt.text(fiducialN, yt, \" N\", color=\"b\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"number of image pixels D\")\n",
        "plt.title(\"varying D at fixed K, N; truncating or extending C\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH_-5JZpcFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make N plot at fiducialD\n",
        "K = bestK\n",
        "D = fiducialD\n",
        "Ns = np.unique(np.round(2 ** np.arange(0., np.log2(maximalN) + 0.01, 0.25)).astype(int))\n",
        "dys_N = np.zeros((len(Ns), 5, Ntrial, Ntest, M)) + np.nan\n",
        "for ii, N in enumerate(Ns):\n",
        "  print(\"starting trials for N =\", N)\n",
        "  C = maximalC[-D-M:, -D-M:]\n",
        "  W = gods_estimator(C, M)\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, -D:]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, -D:]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    P = np.eye(K)[:M, :]\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, P)\n",
        "    G_pca = train_pca_generative_model(xs_train, ys_train, K)\n",
        "    dys_N[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_N[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_N[ii, 2, trial] = ys_test - (G_pca @ xs_test.T).T\n",
        "    dys_N[ii, 3, trial] = ys_test - (W @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSz9OKd1Yjq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_N = np.mean(np.mean(np.mean(dys_N ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_N)\n",
        "y2 = np.nanmax(mses_N) / 0.7\n",
        "yt = np.exp(np.log(y1) + 0.95 * np.log(y2 / y1))\n",
        "plt.fill_between(Ns, mses_N[:, 3], y1 + 0. * mses_N[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(Ns, mses_N[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(Ns, mses_N[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(Ns, mses_N[:, 2], \"go\", mfc=\"none\", ms=10, label=\"PCA generative\")\n",
        "plt.plot(Ns, mses_N[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.axvline(fiducialD, color=\"k\", alpha=0.5)\n",
        "plt.text(fiducialD, yt, \" D\", color=\"k\", alpha=0.5)\n",
        "plt.axvline(K, color=\"r\", alpha=0.5)\n",
        "plt.text(K, yt, \" K\", color=\"r\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"number of training-set objects N\")\n",
        "plt.title(\"varying N at fixed K, D, C\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy1jDPz7N-dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = bestK\n",
        "D = fiducialD\n",
        "N = fiducialN\n",
        "wows = 2. ** np.arange(-2., 10.01, 1.)\n",
        "dys_w = np.zeros((len(wows), 5, Ntrial, Ntest, M))\n",
        "for ii, wow in enumerate(wows):\n",
        "  print(\"starting trials for wow =\", wow)\n",
        "  P = np.eye(K)[:M, :]\n",
        "  C = maximalC[-D-M:, -D-M:]\n",
        "  W = gods_estimator(C, M)\n",
        "  for trial in range(Ntrial):\n",
        "    xs_train = maximalX[trial, :N, -D:]\n",
        "    ys_train = maximalY[trial, :N, :]\n",
        "    xs_test = maximalXtest[trial, :, -D:]\n",
        "    ys_test = maximalYtest[trial, :, :]\n",
        "    B = train_discriminative_model(xs_train, ys_train)\n",
        "    G_msv, zs = train_msv_generative_model(xs_train, ys_train, P)\n",
        "    G_wow, zs = train_wow_generative_model(xs_train, ys_train, P, wow, Z=zs)\n",
        "    G_pca = train_pca_generative_model(xs_train, ys_train, K)\n",
        "    dys_w[ii, 0, trial] = ys_test - (B @ xs_test.T).T\n",
        "    dys_w[ii, 1, trial] = ys_test - (G_msv @ xs_test.T).T\n",
        "    dys_w[ii, 2, trial] = ys_test - (G_pca @ xs_test.T).T\n",
        "    dys_w[ii, 3, trial] = ys_test - (W @ xs_test.T).T\n",
        "    dys_w[ii, 4, trial] = ys_test - (G_wow @ xs_test.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNcMN3xL682v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mses_w = np.mean(np.mean(np.mean(dys_w ** 2, axis=-1), axis=-1), axis=-1)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.subplot(111)\n",
        "y1 = 0.7 * np.nanmin(mses_w)\n",
        "y2 = np.nanmax(mses_w) / 0.7\n",
        "plt.fill_between(wows, mses_w[:, 3], y1 + 0. * mses_w[:, 3], color=\"k\", alpha=0.1)\n",
        "plt.plot(wows, mses_w[:, 0], \"k.\", label=\"discriminative\")\n",
        "plt.plot(wows, mses_w[:, 1], \"ro\", mfc=\"none\", label=\"MSV generative\")\n",
        "plt.plot(wows, mses_w[:, 4], \"gx\", ms=10, label=\"WOW generative\")\n",
        "plt.plot(wows, mses_w[:, 2], \"go\", mfc=\"none\", ms=10, label=\"PCA generative\")\n",
        "plt.plot(wows, mses_w[:, 3], \"k-\", lw=0.5, label=\"best possible MSE\")\n",
        "plt.loglog()\n",
        "plt.legend()\n",
        "plt.ylim(y1, y2)\n",
        "plt.ylabel(\"MSE (variance + bias^2) in prediction of held-out y\")\n",
        "plt.xlabel(\"ratio of weights\")\n",
        "plt.title(\"varying weight ratio at fixed K, D, N, C\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZdEm2Nf70Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}